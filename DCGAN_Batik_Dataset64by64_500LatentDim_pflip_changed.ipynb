{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN_Batik_Dataset64by64_500LatentDim_pflip_changed.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "10_WV2Zn77aobyB54E6BCeJ9gGfR6OyfF",
      "authorship_tag": "ABX9TyMpz6IjUN34TmXKBCXBjVQ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Archangel212/GAN_jupyter_notebooks/blob/master/DCGAN_Batik_Dataset64by64_500LatentDim_pflip_changed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhkE5HVgb4B5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "adaf3340-07fb-428b-9101-96349cd59c61"
      },
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(\"/content/drive/My Drive\")\n",
        "# dcgan on batik dataset\n",
        "# %tensorflow_version 1.0\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential,load_model,save_model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.utils import plot_model\n",
        "import keras.backend as K\n",
        "from matplotlib import pyplot as plt\n",
        "from utils.save_model_summary import save_model_summary\n",
        "from utils.get_last_epoch import get_last_epoch\n",
        "from utils.trim_csv import trim_csv\n",
        "from PIL import ImageFont\n",
        "import io\n",
        "import h5py\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import threading\n",
        "import re\n",
        "import keras\n",
        "import tensorflow as tf \n",
        "\n",
        "np.random.seed(1)\n",
        "tf.random.set_seed(2)\n",
        "\n",
        "print(\"Keras version: %s\" % (keras.__version__))\n",
        "print(\"Tensorflow version: %s\" % (tensorflow.__version__))\n",
        "os.chdir(\"/content/drive/My Drive/DCGAN_Batik_Dataset/AWS_DCGAN_64x64_best_practice_500LatentDim_pflip_changed\")\n",
        "# os.chdir(\"/content/drive/My Drive/DCGAN_Batik_Dataset/test\")\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Keras version: 2.3.1\n",
            "Tensorflow version: 2.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/DCGAN_Batik_Dataset/AWS_DCGAN_64x64_best_practice_500LatentDim_pflip_changed'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIKQCXQCcZFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the standalone discriminator model\n",
        "def define_discriminator(in_shape=(64,64,3)):\n",
        "  init = RandomNormal(mean=0.0,stddev=0.02)\n",
        "  model = Sequential()\n",
        "  # normal\n",
        "  model.add(Conv2D(64, (3,3), padding='same', input_shape=in_shape,kernel_initializer=init))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(axis=-1))\n",
        "  # downsample 32x32\n",
        "  model.add(Conv2D(128, (3,3), strides=(2,2), padding='same',kernel_initializer=init))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(axis=-1))\n",
        "  # downsample 16x16\n",
        "  model.add(Conv2D(128, (3,3), strides=(2,2), padding='same',kernel_initializer=init))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(axis=-1))\n",
        "  # downsample 8x8\n",
        "  model.add(Conv2D(128, (3,3), strides=(2,2), padding='same',kernel_initializer=init))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(axis=-1))\n",
        "  # downsample 4x4\n",
        "  model.add(Conv2D(256, (3,3), strides=(2,2), padding='same',kernel_initializer=init))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(axis=-1))\n",
        "  # classifier\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # compile model\n",
        "  opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1iTJFN-EZi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the standalone generator model\n",
        "def define_generator(latent_dim):\n",
        "  init = RandomNormal(mean=0.0, stddev=0.02)\n",
        "  model = Sequential()\n",
        "  # foundation for 4x4 image\n",
        "  n_nodes = 256 * 4 * 4\n",
        "  model.add(Dense(n_nodes, input_dim=latent_dim, kernel_initializer=init))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(axis=-1))\n",
        "  model.add(Reshape((4, 4, 256)))\n",
        "  # upsample to 8x8\n",
        "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same',kernel_initializer=init))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(axis=-1))\n",
        "  # upsample to 16x16\n",
        "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same',kernel_initializer=init))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(axis=-1))\n",
        "  # upsample to 32x32\n",
        "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same',kernel_initializer=init))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(axis=-1))\n",
        "  # upsample to 64x64\n",
        "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same',kernel_initializer=init))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(axis=-1))\n",
        "  # output layer\n",
        "  model.add(Conv2D(3, (3,3), activation='tanh', padding='same',kernel_initializer=init))\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r25xexrCEcUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# define the combined generator and discriminator model, for updating the generator\n",
        "def define_gan(g_model, d_model):\n",
        "  # make weights in the discriminator not trainable\n",
        "  d_model.trainable = False\n",
        "  # connect them\n",
        "  model = Sequential()\n",
        "  # add generator\n",
        "  model.add(g_model)\n",
        "  # add the discriminator\n",
        "  model.add(d_model)\n",
        "  # compile model\n",
        "  opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHQXfz4CEfOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(ds_path):\n",
        "  with h5py.File(ds_path,\"r\") as f:\n",
        "    dataset = f[\"Batik\"]\n",
        "    dataset = np.copy(dataset)\n",
        "  return dataset\n",
        "\n",
        "# load and prepare cifar10 training images\n",
        "def load_real_samples():\n",
        "\t# load cifar10 dataset\n",
        "  ds_path = \"/content/drive/My Drive/Batik_Datasets/pinterest_version/batik_dataset_64by64.hdf5\"\n",
        "  trainX = load_dataset(ds_path)\n",
        "  # # (trainX, _), (_, _) = load_data()\n",
        "  # convert from unsigned ints to floats\n",
        "  X = trainX.astype('float32')\n",
        "  # scale from [0,255] to [-1,1]\n",
        "  X = (X - 127.5) / 127.5\n",
        "  return X\n",
        "\n",
        "def noisy_labels(y, p_flip):\n",
        "  # ix = np.random.choice(y.shape[0], size=int(y.shape[0]*p_flip), replace=False)\n",
        "  # y[ix] = 1 - y[ix]\n",
        "  n_select = int(p_flip * y.shape[0])\n",
        "  # choose labels to flip\n",
        "  flip_ix = np.random.choice([i for i in range(y.shape[0])], size=n_select, replace=False)\n",
        "  # invert the labels in place\n",
        "  y[flip_ix] = 1 - y[flip_ix]\n",
        "  return y\n",
        "\n",
        "\n",
        "# select real samples\n",
        "def generate_real_samples(dataset, n_samples, label_smoothing=True, label_noising=True, p_flip=0.01):\n",
        "  # choose random instances\n",
        "  ix = randint(0, dataset.shape[0], n_samples)\n",
        "  # retrieve selected images\n",
        "\n",
        "  X = dataset[ix]\n",
        "  # generate 'real' class labels (1)\n",
        "  y = np.ones((n_samples, 1))\n",
        "  if label_smoothing:\n",
        "    y = y - 0.3 + (np.random.random(y.shape) * 0.5) \n",
        "  if label_noising:\n",
        "    y = noisy_labels(y, p_flip)\n",
        "  return X, y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJroeVPRElCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn x_input\n",
        "\n",
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(g_model, latent_dim, n_samples,label_noising=True,p_flip=0.01):\n",
        "\t# generate points in latent space\n",
        "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
        "\t# predict outputs\n",
        "\tX = g_model.predict(x_input)\n",
        "\t# create 'fake' class labels (0)\n",
        "\ty = np.zeros((n_samples, 1))\n",
        "\tif label_noising:\n",
        "\t\ty = noisy_labels(y,p_flip)\n",
        "\treturn X, y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk64Xk71EuSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# evaluate the discriminator, plot generated images, save generator model\n",
        "def summarize_performance(epoch, g_model, d_model, gan_model, dataset, latent_dim, n_samples=150):\n",
        "  # prepare real samples\n",
        "  X_real, y_real = generate_real_samples(dataset, n_samples,label_smoothing=False)\n",
        "  # evaluate discriminator on real examples\n",
        "  _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
        "  # prepare fake examples\n",
        "  x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "  # evaluate discriminator on fake examples\n",
        "  _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
        "  # summarize discriminator performance\n",
        "  print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
        "  # save plot\n",
        "  save_plot(x_fake, epoch)\n",
        "  # save the generator model tile file\n",
        "  gen_filename = 'generator_model_%03d.h5' % (epoch+1)\n",
        "  gen_filename = os.path.join(\"model_checkpoints\",gen_filename)\n",
        "\n",
        "  dis_filename = 'discriminator_model.h5'\n",
        "  dis_filename = os.path.join(\"model_checkpoints\",dis_filename)\n",
        "  \n",
        "  gan_filename = 'GAN_model_weights.h5'\n",
        "  gan_filename = os.path.join(\"model_checkpoints\",gan_filename)\n",
        "\n",
        "  gan_opt_weights_fname = 'GAN_optimizer_weights.npy'\n",
        "  gan_opt_weights_fname = os.path.join(\"model_checkpoints\",gan_opt_weights_fname)\n",
        "\n",
        "\n",
        "  symbolic_weights = gan_model.optimizer.get_weights()\n",
        "  np.save(gan_opt_weights_fname, symbolic_weights)\n",
        "\n",
        "  gan_model.save_weights(gan_filename) \n",
        "  save_model(g_model,gen_filename)\n",
        "  save_model(d_model,dis_filename)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVQa9KdkEpqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# create and save a plot of generated images\n",
        "def save_plot(examples, epoch, n=7):\n",
        "  # scale from [-1,1] to [0,1]\n",
        "  examples = (examples + 1) / 2.0\n",
        "  # plot images\n",
        "  for i in range(n * n):\n",
        "    # define subplot\n",
        "    plt.subplot(n, n, 1 + i)\n",
        "    # turn off axis\n",
        "    plt.axis('off')\n",
        "    # plot raw pixel data\n",
        "    plt.imshow(examples[i])\n",
        "  # save plot to file\n",
        "  filename = 'generated_plot_e%03d.png' % (epoch+1)\n",
        "  filename = os.path.join(\"figure_plots\",filename)\n",
        "  plt.savefig(filename)\n",
        "  plt.close()\n",
        "\n",
        "def plot_history(df_hist):\n",
        "  plt.subplot(2,1,1)\n",
        "  plt.plot(df_hist[\"d_loss_real\"],label=\"d_loss_real\")\n",
        "  plt.plot(df_hist[\"d_loss_fake\"],label=\"d_loss_fake\")\n",
        "  plt.plot(df_hist[\"g_loss\"],label=\"g_loss\")\n",
        "  plt.xlabel(\"number of iterations\")\n",
        "  plt.ylabel(\"loss\")\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(2,1,2)\n",
        "  plt.plot(df_hist[\"d_acc_real\"], label=\"d_acc_real\")\n",
        "  plt.plot(df_hist[\"d_acc_fake\"], label=\"d_acc_fake\")\n",
        "  plt.xlabel(\"number of iterations\")\n",
        "  plt.ylabel(\"accuracy\")\n",
        "  plt.legend()\n",
        "  \n",
        "  plt.savefig(\"accuracy_loss_plot.jpg\")\n",
        "  plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5BfzvMFEwN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# train the generator and discriminator\n",
        "def train(g_model, d_model, gan_model, dataset, latent_dim,csv_path=None, n_epochs=200, n_batch=128, initial_epoch=0):\n",
        "  bat_per_epo = int(dataset.shape[0] / n_batch)\n",
        "  half_batch = int(n_batch / 2)\n",
        "\n",
        "  df_hist = pd.read_csv(csv_path,index_col=None)\n",
        "  list_hist = df_hist.values.tolist()\n",
        "\n",
        "  # manually enumerate epochs\n",
        "  for i in range(initial_epoch,n_epochs):\n",
        "    # enumerate batches over the training set\n",
        "    for j in range(bat_per_epo):\n",
        "      # get randomly selected 'real' samples\n",
        "      X_real, y_real = generate_real_samples(dataset, half_batch,label_smoothing=True)\n",
        "      # update discriminator model weights\n",
        "      d_loss1, d_acc1 = d_model.train_on_batch(X_real, y_real)\n",
        "\n",
        "      # generate 'fake' examples\n",
        "      X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "      # update discriminator model weights\n",
        "      d_loss2, d_acc2 = d_model.train_on_batch(X_fake, y_fake)\n",
        "\n",
        "      # prepare points in latent space as input for the generator\n",
        "      X_gan = generate_latent_points(latent_dim, n_batch)\n",
        "      # create inverted labels for the fake samples\n",
        "      y_gan = np.ones((n_batch, 1))\n",
        "      # update the generator via the discriminator's error\n",
        "      g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "\n",
        "      list_hist.append([i+1, \n",
        "                        \"%d/%d\" % (j+1,bat_per_epo), \n",
        "                        d_acc1, \n",
        "                        d_loss1, \n",
        "                        d_acc2, \n",
        "                        d_loss2,\n",
        "                        g_loss])\n",
        "      \n",
        "\n",
        "      # summarize loss on this batch\n",
        "      print('>%d, %d/%d, d1=%.5f, d2=%.5f, g=%.5f' %\n",
        "        (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
        "    # evaluate the model performance, sometimes\n",
        "\n",
        "    if (i+1) % 5 == 0:\n",
        "      summarize_performance(i, g_model, d_model, gan_model, dataset, latent_dim)\n",
        "      df_hist = pd.DataFrame(list_hist, columns=[\"epoch\",\"batch_per_epo\",\"d_acc_real\",\n",
        "                                                \"d_loss_real\",\"d_acc_fake\",\"d_loss_fake\",\"g_loss\"])\n",
        "      plot_history(df_hist)\n",
        "      df_hist.to_csv(csv_path, mode=\"w\", index=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5vteHgkEx-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(latent_dim=500, initial_epoch=0):\n",
        "  csv_path = \"loss.csv\"\n",
        "  if not os.path.exists(csv_path): \n",
        "    with open(csv_path,'a+') as f:\n",
        "       f.write('epoch,batch_per_epo,d_acc_real,d_loss_real,d_acc_fake,d_loss_fake,g_loss')\n",
        "  os.makedirs('figure_plots', exist_ok=True)\n",
        "  os.makedirs('model_checkpoints', exist_ok=True)\n",
        "  os.makedirs('model_summaries', exist_ok=True)\n",
        "\n",
        "  #get last epoch from  generator last checkpoint\n",
        "  generator_models = list(filter(lambda x: x,[re.findall('[0-9]+',s.split('.')[0]) for s in os.listdir('model_checkpoints')]))\n",
        "  last_epoch = max([int(g[0]) for g in generator_models]) if len(generator_models) != 0 else 0\n",
        "\n",
        "  # size of the latent space is 100 by default\n",
        "  if last_epoch == 0:\n",
        "    print(\"Start training ... \")\n",
        "    # create the generator\n",
        "    g_model = define_generator(latent_dim)\n",
        "    # create the discriminator\n",
        "    d_model = define_discriminator()\n",
        "    # create the gan\n",
        "    gan_model = define_gan(g_model, d_model)\n",
        "    \n",
        "  else:\n",
        "    trim_csv(csv_path, last_epoch)\n",
        "    \n",
        "    model_paths = [\"discriminator_model.h5\",\n",
        "              \"GAN_model_weights.h5\",\"GAN_optimizer_weights.npy\" ]\n",
        "    model_paths = [os.path.join(\"model_checkpoints\", p)  for p in model_paths]\n",
        "    \n",
        "    g_model = load_model(os.path.join(\"model_checkpoints\",\"generator_model_%03d.h5\" % (last_epoch)))\n",
        "    d_model = load_model(model_paths[0])\n",
        "\n",
        "    gan_model = define_gan(g_model,d_model)\n",
        "    gan_model.load_weights(model_paths[1])\n",
        "    gan_model._make_train_function()\n",
        "\n",
        "    weight_optimizer = np.load(model_paths[2], \n",
        "                            allow_pickle=True).tolist()\n",
        "\n",
        "    gan_model.optimizer.set_weights(weight_optimizer)\n",
        "    \n",
        "    initial_epoch = last_epoch\n",
        "    print(\"Last trained epoch %03d\" % (last_epoch))\n",
        "    print(\"Resume training ...\")\n",
        "\n",
        "  # load image data\n",
        "  dataset = load_real_samples()\n",
        "\n",
        "  models = [g_model,d_model,gan_model]\n",
        "  filenames = ['Generator_model.png','Discriminator_model.png','GAN_model.png']\n",
        "  for (model,fn) in zip(models,filenames):\n",
        "    plot_model(model, to_file=fn, show_shapes=True, show_layer_names=True)\n",
        "    save_model_summary(model, os.path.join('model_summaries', fn))\n",
        "  \n",
        "  start = time.time()\n",
        "  train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=1200,\n",
        "        csv_path=csv_path,initial_epoch=initial_epoch)\n",
        "  print(f\"Elapsed time: {time.time() - start} seconds\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3esAZJn5tzD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        },
        "outputId": "7a91faf1-3ce2-46a6-de2a-314497c40532"
      },
      "source": [
        "train_model() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training ... \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">1, 1/112, d1=0.85900, d2=1.10606, g=1.38960\n",
            ">1, 2/112, d1=0.82777, d2=0.05750, g=1.68525\n",
            ">1, 3/112, d1=0.76525, d2=0.03673, g=1.24975\n",
            ">1, 4/112, d1=0.68054, d2=0.12846, g=0.95395\n",
            ">1, 5/112, d1=0.61667, d2=0.11650, g=0.71739\n",
            ">1, 6/112, d1=0.47805, d2=0.13805, g=0.48880\n",
            ">1, 7/112, d1=0.53676, d2=0.02915, g=0.26577\n",
            ">1, 8/112, d1=0.37398, d2=0.03995, g=0.24788\n",
            ">1, 9/112, d1=0.49771, d2=0.02038, g=0.15263\n",
            ">1, 10/112, d1=0.47369, d2=0.00891, g=0.16371\n",
            ">1, 11/112, d1=0.38113, d2=0.00522, g=0.12574\n",
            ">1, 12/112, d1=0.34190, d2=0.00366, g=0.07184\n",
            ">1, 13/112, d1=0.37070, d2=0.00476, g=0.05470\n",
            ">1, 14/112, d1=0.30897, d2=0.00329, g=0.03540\n",
            ">1, 15/112, d1=0.31333, d2=0.00467, g=0.05258\n",
            ">1, 16/112, d1=0.42679, d2=0.00133, g=0.02468\n",
            ">1, 17/112, d1=0.31079, d2=0.00079, g=0.02882\n",
            ">1, 18/112, d1=0.25867, d2=0.00279, g=0.02953\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c3162e7c9a7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-4da05fc6bc89>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(latent_dim, initial_epoch)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m   train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=1200,\n\u001b[0;32m---> 58\u001b[0;31m         csv_path=csv_path,initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Elapsed time: {time.time() - start} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-b81c2ee7e561>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(g_model, d_model, gan_model, dataset, latent_dim, csv_path, n_epochs, n_batch, initial_epoch)\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0my_gan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0;31m# update the generator via the discriminator's error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m       \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_gan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_gan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m       list_hist.append([i+1, \n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3798\u001b[0m     return nest.pack_sequence_as(\n\u001b[1;32m   3799\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3800\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3801\u001b[0m         expand_composites=True)\n\u001b[1;32m   3802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3798\u001b[0m     return nest.pack_sequence_as(\n\u001b[1;32m   3799\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3800\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3801\u001b[0m         expand_composites=True)\n\u001b[1;32m   3802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6AnN-bWF0et",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import os\n",
        "from utils.get_last_epoch import get_last_epoch\n",
        "\n",
        "ckpt_paths = [\"generator_model_%03d.h5\",\"discriminator_model_%03d.h5\",\n",
        "              \"GAN_model_weights_%03d.h5\",\"GAN_optimizer_weights_%03d.npy\" ]\n",
        "last_epoch = get_last_epoch('model_checkpoints')\n",
        "ckpt_paths = [os.path.join(\"model_checkpoints\", p % (last_epoch))  for p in ckpt_paths]\n",
        "ckpt_paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcF2-gQJd9nU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import pandas as pd\n",
        "# csv = pd.read_csv(\"accuracy_loss.csv\")\n",
        "# csv.shape,csv.iloc[-112:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuX_yAlOpbGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gan_model.load_weights(\"/content/drive/My Drive/DCGAN_Batik_Dataset/test/gan_model_weights.h5\")\n",
        "gan_model._make_train_function()\n",
        "\n",
        "with open('optimizer.pkl', 'rb') as f:\n",
        "  weight_values = pickle.load(f)\n",
        "\n",
        "gan_model.optimizer.set_weights(weight_values)\n",
        "# gan_optimizer_weights = np.load(\"/content/drive/My Drive/DCGAN_Batik_Dataset/test/gan_optimizer_weights/gan_optimizer_weights.npy\",allow_pickle=True)\n",
        "# gan_model.optimizer.set_weights(gan_optimizer_weights.tolist())\n",
        "# len(gan_model.optimizer.weights),len(gan_optimizer_weights.tolist()), len(weight_values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bkn5PJypo-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# l = gan_model.optimizer.get_weights()\n",
        "# arr = np.asarray(l)\n",
        "# np.save('optimizer_weights.npy', arr)\n",
        "# arr == l\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6x8F08Bp34N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loaded_arr = np.load('optimizer_weights.npy',allow_pickle=True)\n",
        "# loaded_arr = loaded_arr.tolist()\n",
        "# for i in range(len(loaded_arr)):\n",
        "#   is_equal = np.array_equal(arr[0],l[0])\n",
        "#   if not is_equal:\n",
        "#     print(i)\n",
        "# gan_model.optimizer.set_weights(loaded_arr)\n",
        "# gan_model.optimizer.get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsFKxrwVPrV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ckpt_path = \"model_checkpoints\"\n",
        "# discriminator_ckpt = list(map(lambda x: x.group(),filter(lambda x : x,[re.match(\"discriminator_model_[0-9]+.h5\",m) for m in os.listdir(ckpt_path)])))\n",
        "# discriminator_ckpt = list(filter(lambda x: x != \"discriminator_model_400.h5\" ,discriminator_ckpt))\n",
        "# GAN_model_weights_ckpt = list(map(lambda x: x.group(),filter(lambda x : x,[re.match(\"GAN_model_weights_[0-9]+.h5\",m) for m in os.listdir(ckpt_path)])))\n",
        "# GAN_model_weights_ckpt = list(filter(lambda x: x != \"GAN_model_weights_400.h5\" ,GAN_model_weights_ckpt))\n",
        "# GAN_optimizer_weights_ckpt = list(map(lambda x: x.group(),filter(lambda x : x,[re.match(\"GAN_optimizer_weights_[0-9]+.npy\",m) for m in os.listdir(ckpt_path)])))\n",
        "# GAN_optimizer_weights_ckpt = list(filter(lambda x: x != \"GAN_optimizer_weights_400.npy\" ,GAN_optimizer_weights_ckpt))\n",
        "\n",
        "# for fn in zip(discriminator_ckpt, GAN_model_weights_ckpt, GAN_optimizer_weights_ckpt):\n",
        "#   print(fn)\n",
        "  # os.remove(os.path.join(ckpt_path,fn[0]))\n",
        "  # os.remove(os.path.join(ckpt_path,fn[1]))\n",
        "  # os.remove(os.path.join(ckpt_path,fn[2]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vpb8tHMl00H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generator_models = filter(lambda x: x,[re.findall('[0-9]+',s.split('.')[0]) for s in os.listdir('model_checkpoints')])\n",
        "# last_epoch = max([int(g[0]) for g in generator_models])\n",
        "# model_paths = [\"discriminator_model.h5\",\n",
        "#               \"GAN_model_weights.h5\",\"GAN_optimizer_weights.npy\" ]\n",
        "# model_paths = [os.path.join(\"model_checkpoints\", p)  for p in model_paths]\n",
        "# g_model = load_model(os.path.join(\"model_checkpoints\",\"generator_model_%03d.h5\" % (last_epoch)))\n",
        "# d_model = load_model(model_paths[0])\n",
        "\n",
        "# gan_model = define_gan(g_model,d_model)\n",
        "# gan_model.load_weights(model_paths[1])\n",
        "# gan_model._make_train_function()\n",
        "\n",
        "# weight_optimizer = np.load(model_paths[2], \n",
        "#                         allow_pickle=True).tolist()\n",
        "\n",
        "# gan_model.optimizer.set_weights(weight_optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9mTXvOGpXAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}