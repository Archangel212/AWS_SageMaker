{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN_BATIK_Dataset_64by64_UNet_architecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1X8eQnyh1eab4Ct9e64PRGohUyoCqFjPw",
      "authorship_tag": "ABX9TyPlrjS958SmjZO7aPk+ht55",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Archangel212/GAN_jupyter_notebooks/blob/master/WGAN_BATIK_Dataset_64by64_UNet_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMTZp16sdxpO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e7caa4d-876d-4c49-b959-5846a3d0b870"
      },
      "source": [
        "# example of a wgan for generating handwritten digits\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive')\n",
        "\n",
        "from numpy import expand_dims\n",
        "from numpy import mean\n",
        "from numpy import ones\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.datasets.mnist import load_data\n",
        "from keras import backend\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.models import Sequential,save_model,load_model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.constraints import Constraint\n",
        "from keras.utils import plot_model\n",
        "from matplotlib import pyplot\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import re\n",
        "import h5py\n",
        "import pandas as pd\n",
        "from utils.save_model_summary import save_model_summary\n",
        "from utils.trim_csv import trim_csv\n",
        "import math\n",
        "import tensorflow as tf \n",
        "\n",
        "np.random.seed(1)\n",
        "tf.random.set_seed(2)\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/WGAN/BATIK_Dataset_64by64_UNet_architecture\")\n",
        "os.getcwd()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/WGAN/BATIK_Dataset_64by64_UNet_architecture'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SpVfhzLfwPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clip model weights to a given hypercube\n",
        "class ClipConstraint(Constraint):\n",
        "\t# set clip value when initialized\n",
        "\tdef __init__(self, clip_value):\n",
        "\t\tself.clip_value = clip_value\n",
        "\n",
        "\t# clip model weights to hypercube\n",
        "\tdef __call__(self, weights):\n",
        "\t\treturn backend.clip(weights, -self.clip_value, self.clip_value)\n",
        "\n",
        "\t# get the config\n",
        "\tdef get_config(self):\n",
        "\t\treturn {'clip_value': self.clip_value}\n",
        "\n",
        "# calculate wasserstein loss\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "\treturn backend.mean(y_true * y_pred)\n",
        "\n",
        "# define the standalone critic model\n",
        "def define_critic(in_shape=(64,64,3)):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# weight constraint\n",
        "\tconst = ClipConstraint(0.01)\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\n",
        "\t# input layer\n",
        "\tmodel.add(Conv2D(64, (3,3), padding='same', input_shape=in_shape, kernel_initializer=init, kernel_constraint=const))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# downsample to 32x32\n",
        "\tmodel.add(Conv2D(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init, kernel_constraint=const))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# downsample to 16x16\n",
        "\tmodel.add(Conv2D(128, (3,3), strides=(2,2), padding='same',kernel_initializer=init, kernel_constraint=const))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# downsample to 8x8\n",
        "\tmodel.add(Conv2D(256, (3,3), strides=(2,2), padding='same',kernel_initializer=init, kernel_constraint=const))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# downsample to 4x4\n",
        "\tmodel.add(Conv2D(512, (3,3), strides=(2,2), padding='same',kernel_initializer=init, kernel_constraint=const))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# downsample to 2x2\n",
        "\tmodel.add(Conv2D(512, (3,3), strides=(2,2), padding='same',kernel_initializer=init, kernel_constraint=const))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# downsample to 1x1\n",
        "\tmodel.add(Conv2D(512, (3,3), strides=(2,2), padding='same',kernel_initializer=init, kernel_constraint=const))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# scoring, linear activation\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(1))\n",
        "\t# compile model\n",
        "\topt = RMSprop(lr=0.00005)\n",
        "\tmodel.compile(loss=wasserstein_loss, optimizer=opt)\n",
        "\treturn model\n",
        "\n",
        "# define the standalone generator model\n",
        "def define_generator(latent_dim):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\t# foundation for 2x2 image\n",
        "\tn_nodes = 512 * 2 * 2\n",
        "\tmodel.add(Dense(n_nodes, kernel_initializer=init, input_dim=latent_dim))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\tmodel.add(Reshape((2, 2, 512)))\n",
        "\t# upsample to 4x4\n",
        "\tmodel.add(Conv2DTranspose(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# upsample to 8x8\n",
        "\tmodel.add(Conv2DTranspose(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# upsample to 16x16\n",
        "\tmodel.add(Conv2DTranspose(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# upsample to 32x32\n",
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same',kernel_initializer=init))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# upsample to 64x64\n",
        "\tmodel.add(Conv2DTranspose(64, (4,4), strides=(2,2), padding='same',kernel_initializer=init))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# output 64x64x3\n",
        "\tmodel.add(Conv2D(3, (3,3), activation='tanh', padding='same', kernel_initializer=init))\n",
        "\treturn model\n",
        "\n",
        "# define the combined generator and critic model, for updating the generator\n",
        "def define_gan(generator, critic):\n",
        "\t# make weights in the critic not trainable\n",
        "\tcritic.trainable = False\n",
        "\t# connect them\n",
        "\tmodel = Sequential()\n",
        "\t# add generator\n",
        "\tmodel.add(generator)\n",
        "\t# add the critic\n",
        "\tmodel.add(critic)\n",
        "\t# compile model\n",
        "\topt = RMSprop(lr=0.00005)\n",
        "\tmodel.compile(loss=wasserstein_loss, optimizer=opt)\n",
        "\treturn model\n",
        "\n",
        "\n",
        "def load_dataset(ds_path):\n",
        "  with h5py.File(ds_path,\"r\") as f:\n",
        "    dataset = f[\"Batik\"]\n",
        "    dataset = np.copy(dataset)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "\t\n",
        "# load images\n",
        "def load_real_samples():\n",
        "\t# load cifar10 dataset\n",
        "\tds_path = \"/content/drive/My Drive/Batik_Datasets/pinterest_version/batik_dataset_64by64.hdf5\"\n",
        "\ttrainX = load_dataset(ds_path)\n",
        "\t# convert from unsigned ints to floats\n",
        "\tX = trainX.astype('float32')\n",
        "\t# scale from [0,255] to [-1,1]\n",
        "\tX = (X - 127.5) / 127.5\n",
        "\treturn X\n",
        "\n",
        "def noisy_labels(y, p_flip):\n",
        "\t# ix = np.random.choice(y.shape[0], size=int(y.shape[0]*p_flip), replace=False)\n",
        "\t# y[ix] = 1 - y[ix]\n",
        "\tn_select = int(p_flip * y.shape[0])\n",
        "\t# choose labels to flip\n",
        "\tflip_ix = np.random.choice([i for i in range(y.shape[0])], size=n_select,replace=False)\n",
        "\t# invert the labels in place\n",
        "\ty[flip_ix] =  -1 *  y[flip_ix]\n",
        "\treturn y\n",
        "\n",
        "# select real samples\n",
        "def generate_real_samples(dataset, n_samples, label_noising=True, p_flip=0.05):\n",
        "\t# choose random instances\n",
        "\tix = randint(0, dataset.shape[0], n_samples)\n",
        "\t# select images\n",
        "\tX = dataset[ix]\n",
        "\t# generate class labels, -1 for 'real'\n",
        "\ty = -ones((n_samples, 1))\n",
        " \n",
        "\tif label_noising:\n",
        "\t\ty = noisy_labels(y, p_flip) \n",
        "\t\n",
        "\treturn X, y\n",
        "\n",
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn x_input\n",
        "\n",
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(generator, latent_dim, n_samples, label_noising=True, p_flip=0.05):\n",
        "\t# generate points in latent space\n",
        "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
        "\t# predict outputs\n",
        "\tX = generator.predict(x_input)\n",
        "\t# create class labels with 1.0 for 'fake'\n",
        "\ty = ones((n_samples, 1))\n",
        " \n",
        "\tif label_noising:\n",
        "\t\ty = noisy_labels(y, p_flip)\n",
        "\t\n",
        "\treturn X, y\n",
        "\n",
        "# generate samples and save as a plot and save the model\n",
        "def summarize_performance(epoch,c_model, g_model, gan_model, latent_dim,n_samples=100):\n",
        "\n",
        "\t# prepare fake examples\n",
        "\tX, _ = generate_fake_samples(g_model, latent_dim, n_samples, label_noising=False)\n",
        "\t# scale from [-1,1] to [0,1]\n",
        "\tX = (X + 1) / 2.0\n",
        "\t# plot images\n",
        "\tfor i in range(7 * 7):\n",
        "\t\t# define subplot\n",
        "\t\tpyplot.subplot(7, 7, 1 + i)\n",
        "\t\t# turn off axis\n",
        "\t\tpyplot.axis('off')\n",
        "\t\t# plot raw pixel data\n",
        "\t\tpyplot.imshow(X[i])\n",
        "\t# save plot to file\n",
        "\tfigure_plot = os.path.join('figure_plots','generated_plot_%03d.png' % (epoch))\n",
        "\tpyplot.savefig(figure_plot)\n",
        "\tpyplot.close()\n",
        " \n",
        "\t#save critic model\n",
        "\tcritic_model_weights_fname = os.path.join('model_checkpoints','critic_model_weights.h5')\n",
        "\tc_model.save_weights(critic_model_weights_fname)\n",
        "\n",
        "\tgenerator_fname = os.path.join('model_checkpoints', 'generator_model_%03d.h5' % (epoch))\n",
        "\tsave_model(g_model, generator_fname)\n",
        "\n",
        "\tgan_model_weights_fname = os.path.join('model_checkpoints', 'GAN_model_weights.h5')\n",
        "\tgan_model.save_weights(gan_model_weights_fname)\n",
        "\n",
        "\tgan_optimizer_weights_fname = os.path.join('model_checkpoints', 'GAN_optimizer_weights.npy')\n",
        "\tgan_optimizer_weights = gan_model.optimizer.get_weights()\n",
        "\tnp.save(gan_optimizer_weights_fname, gan_optimizer_weights, allow_pickle=True)\n",
        "\n",
        "\tprint(f'Model_epoch_{epoch} saved')\n",
        "\n",
        "# create a line plot of loss for the gan and save to file\n",
        "def plot_history(df_hist):\n",
        "\t# plot history\n",
        "\tpyplot.plot(df_hist[\"critic_loss_real\"], label='critic_real')\n",
        "\tpyplot.plot(df_hist[\"critic_loss_fake\"], label='critic_fake')\n",
        "\tpyplot.plot(df_hist[\"gan_loss\"], label='gen')\n",
        "\tpyplot.xlabel(\"number of iterations\")\n",
        "\tpyplot.ylabel(\"loss\")\n",
        "\tpyplot.legend()\n",
        "\tpyplot.savefig('plot_line_plot_loss.png')\n",
        "\tpyplot.close()\n",
        "\n",
        "\n",
        "# train the generator and critic\n",
        "def train(g_model, c_model, gan_model, dataset, latent_dim, csv_path=None, n_epochs=10, n_batch=64, n_critic=5, initial_epoch=0):\n",
        "\t# calculate the number of batches per training epoch\n",
        "\tbat_per_epo = int(dataset.shape[0] / n_batch)\n",
        "\tinitial_step = initial_epoch*bat_per_epo\n",
        "\n",
        "\tprint(dataset.shape,\"bat_per_epo: \", bat_per_epo)\n",
        "\t# calculate the number of training iterations\n",
        "\tn_steps = bat_per_epo * n_epochs\n",
        "\t# calculate the size of half a batch of samples\n",
        "\thalf_batch = int(n_batch / 2)\n",
        "\t# lists for keeping track of loss\n",
        "\t# c1_hist, c2_hist, g_hist = list(), list(), list()\n",
        "\tdf_hist = pd.read_csv(csv_path,index_col=None)\n",
        "\tlist_hist = df_hist.values.tolist()\n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(initial_step, n_steps):\n",
        "\t\t# update the critic more than the generator\n",
        "\t\tepoch = math.ceil((i+1)/bat_per_epo)\n",
        "\t\tsteps = \"%d/%d\" % ((i+1)%bat_per_epo if (i+1)%bat_per_epo != 0 else bat_per_epo ,bat_per_epo)  \n",
        "\n",
        "\t\tc1_tmp, c2_tmp = list(), list()\n",
        "\t\tfor _ in range(n_critic):\n",
        "\t\t\t# get randomly selected 'real' samples\n",
        "\t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "\t\t\t# update critic model weights\n",
        "\t\t\tc_loss1 = c_model.train_on_batch(X_real, y_real)\n",
        "\t\t\tc1_tmp.append(c_loss1)\n",
        "\t\t\t# generate 'fake' examples\n",
        "\t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "\t\t\t# update critic model weights\n",
        "\t\t\tc_loss2 = c_model.train_on_batch(X_fake, y_fake)\n",
        "\t\t\tc2_tmp.append(c_loss2)\n",
        "\t\t# prepare points in latent space as input for the generator\n",
        "\t\tX_gan = generate_latent_points(latent_dim, n_batch)\n",
        "\t\t# create inverted labels for the fake samples\n",
        "\t\ty_gan = -ones((n_batch, 1))\n",
        "\t\t# update the generator via the critic's error\n",
        "\t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "\t\t# g_hist.append(g_loss)\n",
        "\t\tlist_hist.append([epoch, steps, mean(c1_tmp), mean(c2_tmp),g_loss])\n",
        "\t\t# df_hist = df_hist.append({\n",
        "\t\t# \t\t\"epoch\"\t: epoch,\n",
        "\t\t# \t\t\"steps\" : steps,\n",
        "\t\t# \t\t\"critic_loss_real\" : mean(c1_tmp),\n",
        "\t\t# \t\t\"critic_loss_fake\" : mean(c2_tmp),\n",
        "\t\t# \t\t\"gan_loss\" : g_loss\n",
        "\t\t# }, ignore_index=True)\n",
        "\n",
        "\n",
        "\t\t# summarize loss on this batch\n",
        "\t\tprint('>%d, %s, c1=%.3f, c2=%.3f g=%.3f' % (epoch, steps, mean(c1_tmp), mean(c2_tmp),g_loss))\n",
        "\t\t# evaluate the model performance every '2 epoch'\n",
        "\t\tif (i+1) % (bat_per_epo*2) == 0 or (i+1) == bat_per_epo:\n",
        "\t\t\tsummarize_performance(epoch, c_model, g_model, gan_model, latent_dim)\n",
        "\t\t\tdf_hist = pd.DataFrame(list_hist, columns=[\"epoch\",\"steps\",\"critic_loss_real\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"critic_loss_fake\",\"gan_loss\"])\n",
        "\t\t\tplot_history(df_hist)\n",
        "\t\t\tdf_hist.to_csv(csv_path, mode=\"w\", index=False)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdeyWY1jhTii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train_model(initial_epoch=0, latent_dim=250):\n",
        "  csv_path = \"loss.csv\"\n",
        "  if not os.path.exists(csv_path): \n",
        "    with open(csv_path,'a+') as f:\n",
        "      f.write('epoch,steps,critic_loss_real,critic_loss_fake,gan_loss')\n",
        "  os.makedirs('figure_plots', exist_ok=True)\n",
        "  os.makedirs('model_checkpoints', exist_ok=True)\n",
        "  os.makedirs('model_summaries', exist_ok=True)\n",
        "  \n",
        "  #get last epoch from  generator last checkpoint\n",
        "  generator_models = list(filter(lambda x: x,[re.findall('[0-9]+',s.split('.')[0]) for s in os.listdir('model_checkpoints')]))\n",
        "  last_epoch = max([int(g[0]) for g in generator_models]) if len(generator_models) != 0 else 0\n",
        "\n",
        "  if last_epoch == 0:\n",
        "    print(\"Start training ... \")\n",
        "    # create the critic\n",
        "    c_model = define_critic()\n",
        "    # create the generator\n",
        "    g_model = define_generator(latent_dim)\n",
        "    # create the gan\n",
        "    gan_model = define_gan(g_model, c_model)\n",
        "\n",
        "  else:\n",
        "    trim_csv(csv_path, last_epoch)\n",
        "    \n",
        "    model_paths = [\"critic_model_weights.h5\",\"GAN_model_weights.h5\",\"GAN_optimizer_weights.npy\"]\n",
        "    model_filenames = [os.path.join(\"model_checkpoints\", p)  for p in model_paths] \n",
        "\n",
        "    g_model = load_model(os.path.join(\"model_checkpoints\",\"generator_model_%03d.h5\" % (last_epoch)))\n",
        "    c_model = define_critic()\n",
        "    c_model.load_weights(model_filenames[0])\n",
        "\n",
        "    gan_model = define_gan(g_model, c_model)\n",
        "    gan_model.load_weights(model_filenames[1])\n",
        "    gan_model._make_train_function()\n",
        "\n",
        "    gan_model_optimizer_weights = np.load(model_filenames[2], allow_pickle=True).tolist()\n",
        "    gan_model.optimizer.set_weights(gan_model_optimizer_weights)\n",
        "\n",
        "    initial_epoch = last_epoch    \n",
        "    print(f\"Last trained model at epoch : {last_epoch}\")\n",
        "    print(\"Resume Training ...\")\n",
        "\n",
        "  # load image data\n",
        "  dataset = load_real_samples()\n",
        "  \n",
        "  models = [g_model,c_model,gan_model]\n",
        "  filenames = ['Generator_model.png','Critic_model.png','GAN_model.png']\n",
        "  for (model,fn) in zip(models,filenames):\n",
        "    plot_model(model, to_file=fn, show_shapes=True, show_layer_names=True)\n",
        "    save_model_summary(model, os.path.join('model_summaries', fn))\n",
        "\n",
        "  start = time.time()\n",
        "  train(g_model, c_model, gan_model, dataset, latent_dim, n_epochs=1200,\n",
        "        csv_path=csv_path,initial_epoch=initial_epoch)\n",
        "  print(f\"Elapsed time: {time.time() - start} seconds\") "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdxnx6T8gLkq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3441cc55-991c-4294-bcdf-cbdcecd905bd"
      },
      "source": [
        "train_model()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "csv trimed at epoch > 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Last trained model at epoch : 8\n",
            "Resume Training ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(14400, 64, 64, 3) bat_per_epo:  225\n",
            ">9, 1/225, c1=-1.027, c2=-1.093 g=-0.011\n",
            ">9, 2/225, c1=2.894, c2=-0.236 g=-0.011\n",
            ">9, 3/225, c1=-0.436, c2=-1.742 g=-0.012\n",
            ">9, 4/225, c1=0.664, c2=-1.013 g=-0.012\n",
            ">9, 5/225, c1=-1.705, c2=-0.009 g=-0.012\n",
            ">9, 6/225, c1=-0.880, c2=2.394 g=-0.012\n",
            ">9, 7/225, c1=0.752, c2=-1.621 g=-0.012\n",
            ">9, 8/225, c1=-1.392, c2=2.445 g=-0.012\n",
            ">9, 9/225, c1=0.519, c2=-2.208 g=-0.012\n",
            ">9, 10/225, c1=-1.534, c2=-0.011 g=-0.012\n",
            ">9, 11/225, c1=1.774, c2=-0.338 g=-0.012\n",
            ">9, 12/225, c1=-1.901, c2=-0.814 g=-0.012\n",
            ">9, 13/225, c1=-1.155, c2=-0.961 g=-0.012\n",
            ">9, 14/225, c1=1.891, c2=3.502 g=-0.012\n",
            ">9, 15/225, c1=0.262, c2=-1.994 g=-0.012\n",
            ">9, 16/225, c1=0.744, c2=0.429 g=-0.012\n",
            ">9, 17/225, c1=-0.434, c2=0.427 g=-0.012\n",
            ">9, 18/225, c1=0.177, c2=0.590 g=-0.012\n",
            ">9, 19/225, c1=-0.430, c2=2.786 g=-0.012\n",
            ">9, 20/225, c1=-0.792, c2=1.907 g=-0.012\n",
            ">9, 21/225, c1=1.207, c2=0.173 g=-0.012\n",
            ">9, 22/225, c1=0.118, c2=0.844 g=-0.012\n",
            ">9, 23/225, c1=0.512, c2=-0.889 g=-0.012\n",
            ">9, 24/225, c1=-1.590, c2=-0.621 g=-0.012\n",
            ">9, 25/225, c1=0.171, c2=-0.034 g=-0.012\n",
            ">9, 26/225, c1=-0.791, c2=1.019 g=-0.012\n",
            ">9, 27/225, c1=0.034, c2=-0.904 g=-0.012\n",
            ">9, 28/225, c1=-0.738, c2=-0.410 g=-0.012\n",
            ">9, 29/225, c1=0.840, c2=0.632 g=-0.012\n",
            ">9, 30/225, c1=0.256, c2=0.313 g=-0.012\n",
            ">9, 31/225, c1=-0.412, c2=0.904 g=-0.012\n",
            ">9, 32/225, c1=-0.935, c2=-1.677 g=-0.012\n",
            ">9, 33/225, c1=0.125, c2=0.331 g=-0.012\n",
            ">9, 34/225, c1=-0.169, c2=-0.085 g=-0.012\n",
            ">9, 35/225, c1=-0.615, c2=-1.487 g=-0.012\n",
            ">9, 36/225, c1=-0.981, c2=1.578 g=-0.012\n",
            ">9, 37/225, c1=-0.619, c2=0.384 g=-0.012\n",
            ">9, 38/225, c1=-0.315, c2=-0.052 g=-0.012\n",
            ">9, 39/225, c1=1.114, c2=-1.749 g=-0.012\n",
            ">9, 40/225, c1=-0.038, c2=1.099 g=-0.012\n",
            ">9, 41/225, c1=-0.664, c2=0.545 g=-0.012\n",
            ">9, 42/225, c1=3.610, c2=2.809 g=-0.012\n",
            ">9, 43/225, c1=0.334, c2=0.985 g=-0.012\n",
            ">9, 44/225, c1=0.611, c2=-0.825 g=-0.012\n",
            ">9, 45/225, c1=-1.028, c2=-0.110 g=-0.012\n",
            ">9, 46/225, c1=1.987, c2=-0.008 g=-0.012\n",
            ">9, 47/225, c1=-0.813, c2=-2.059 g=-0.012\n",
            ">9, 48/225, c1=1.999, c2=-0.962 g=-0.012\n",
            ">9, 49/225, c1=0.279, c2=-1.036 g=-0.012\n",
            ">9, 50/225, c1=0.785, c2=-1.268 g=-0.012\n",
            ">9, 51/225, c1=-1.295, c2=-0.505 g=-0.012\n",
            ">9, 52/225, c1=0.936, c2=3.261 g=-0.012\n",
            ">9, 53/225, c1=-1.656, c2=1.343 g=-0.012\n",
            ">9, 54/225, c1=0.745, c2=0.375 g=-0.012\n",
            ">9, 55/225, c1=0.541, c2=0.131 g=-0.012\n",
            ">9, 56/225, c1=0.854, c2=-0.747 g=-0.012\n",
            ">9, 57/225, c1=-0.980, c2=1.339 g=-0.012\n",
            ">9, 58/225, c1=-1.533, c2=3.487 g=-0.012\n",
            ">9, 59/225, c1=-1.859, c2=1.469 g=-0.012\n",
            ">9, 60/225, c1=0.162, c2=1.385 g=-0.012\n",
            ">9, 61/225, c1=0.390, c2=2.601 g=-0.012\n",
            ">9, 62/225, c1=2.087, c2=1.495 g=-0.012\n",
            ">9, 63/225, c1=-1.670, c2=0.825 g=-0.012\n",
            ">9, 64/225, c1=0.088, c2=-1.102 g=-0.012\n",
            ">9, 65/225, c1=0.467, c2=-0.477 g=-0.012\n",
            ">9, 66/225, c1=0.746, c2=-0.865 g=-0.012\n",
            ">9, 67/225, c1=-0.718, c2=0.548 g=-0.012\n",
            ">9, 68/225, c1=2.220, c2=0.625 g=-0.012\n",
            ">9, 69/225, c1=-1.565, c2=-1.110 g=-0.012\n",
            ">9, 70/225, c1=1.088, c2=1.681 g=-0.012\n",
            ">9, 71/225, c1=-0.215, c2=-0.229 g=-0.012\n",
            ">9, 72/225, c1=0.174, c2=1.837 g=-0.011\n",
            ">9, 73/225, c1=-1.246, c2=-0.249 g=-0.012\n",
            ">9, 74/225, c1=-1.678, c2=3.261 g=-0.011\n",
            ">9, 75/225, c1=-2.265, c2=0.489 g=-0.012\n",
            ">9, 76/225, c1=-0.867, c2=-1.357 g=-0.012\n",
            ">9, 77/225, c1=-0.467, c2=-0.868 g=-0.012\n",
            ">9, 78/225, c1=-0.387, c2=-1.400 g=-0.012\n",
            ">9, 79/225, c1=2.313, c2=-0.442 g=-0.012\n",
            ">9, 80/225, c1=-0.973, c2=1.069 g=-0.012\n",
            ">9, 81/225, c1=-0.543, c2=-0.419 g=-0.012\n",
            ">9, 82/225, c1=1.560, c2=-1.692 g=-0.012\n",
            ">9, 83/225, c1=1.722, c2=1.312 g=-0.012\n",
            ">9, 84/225, c1=-0.710, c2=-0.492 g=-0.012\n",
            ">9, 85/225, c1=1.021, c2=1.048 g=-0.012\n",
            ">9, 86/225, c1=2.845, c2=-0.667 g=-0.012\n",
            ">9, 87/225, c1=-1.529, c2=1.883 g=-0.012\n",
            ">9, 88/225, c1=-1.186, c2=1.951 g=-0.012\n",
            ">9, 89/225, c1=0.346, c2=0.024 g=-0.012\n",
            ">9, 90/225, c1=-0.464, c2=0.700 g=-0.012\n",
            ">9, 91/225, c1=3.326, c2=0.201 g=-0.011\n",
            ">9, 92/225, c1=-1.558, c2=0.957 g=-0.011\n",
            ">9, 93/225, c1=-1.901, c2=1.545 g=-0.011\n",
            ">9, 94/225, c1=1.411, c2=-1.118 g=-0.011\n",
            ">9, 95/225, c1=0.157, c2=0.818 g=-0.011\n",
            ">9, 96/225, c1=-0.870, c2=0.166 g=-0.011\n",
            ">9, 97/225, c1=-1.163, c2=-0.812 g=-0.012\n",
            ">9, 98/225, c1=-1.187, c2=-1.907 g=-0.012\n",
            ">9, 99/225, c1=-1.190, c2=-0.505 g=-0.012\n",
            ">9, 100/225, c1=-0.582, c2=-0.601 g=-0.012\n",
            ">9, 101/225, c1=-1.140, c2=1.004 g=-0.012\n",
            ">9, 102/225, c1=1.095, c2=-0.699 g=-0.012\n",
            ">9, 103/225, c1=1.662, c2=-0.720 g=-0.012\n",
            ">9, 104/225, c1=-1.447, c2=0.128 g=-0.012\n",
            ">9, 105/225, c1=-1.329, c2=-0.501 g=-0.012\n",
            ">9, 106/225, c1=-1.106, c2=2.283 g=-0.012\n",
            ">9, 107/225, c1=0.281, c2=0.584 g=-0.012\n",
            ">9, 108/225, c1=1.541, c2=-1.940 g=-0.012\n",
            ">9, 109/225, c1=-1.133, c2=0.054 g=-0.012\n",
            ">9, 110/225, c1=2.166, c2=-0.332 g=-0.012\n",
            ">9, 111/225, c1=-0.986, c2=-0.725 g=-0.012\n",
            ">9, 112/225, c1=-0.543, c2=1.155 g=-0.012\n",
            ">9, 113/225, c1=0.111, c2=-1.639 g=-0.012\n",
            ">9, 114/225, c1=-1.359, c2=0.975 g=-0.012\n",
            ">9, 115/225, c1=-0.297, c2=-0.231 g=-0.012\n",
            ">9, 116/225, c1=1.274, c2=1.331 g=-0.012\n",
            ">9, 117/225, c1=-1.054, c2=-1.377 g=-0.012\n",
            ">9, 118/225, c1=-1.706, c2=-0.789 g=-0.012\n",
            ">9, 119/225, c1=-1.522, c2=-0.095 g=-0.012\n",
            ">9, 120/225, c1=2.276, c2=1.186 g=-0.012\n",
            ">9, 121/225, c1=-0.889, c2=2.039 g=-0.012\n",
            ">9, 122/225, c1=-0.789, c2=0.101 g=-0.012\n",
            ">9, 123/225, c1=1.293, c2=0.183 g=-0.012\n",
            ">9, 124/225, c1=0.065, c2=0.423 g=-0.012\n",
            ">9, 125/225, c1=0.298, c2=-0.613 g=-0.012\n",
            ">9, 126/225, c1=-0.020, c2=-0.180 g=-0.012\n",
            ">9, 127/225, c1=-0.454, c2=-0.980 g=-0.012\n",
            ">9, 128/225, c1=-1.144, c2=-0.707 g=-0.012\n",
            ">9, 129/225, c1=-0.398, c2=-1.996 g=-0.012\n",
            ">9, 130/225, c1=3.622, c2=0.571 g=-0.012\n",
            ">9, 131/225, c1=0.851, c2=-2.818 g=-0.012\n",
            ">9, 132/225, c1=0.341, c2=-1.145 g=-0.012\n",
            ">9, 133/225, c1=-0.711, c2=-2.655 g=-0.012\n",
            ">9, 134/225, c1=-0.288, c2=-2.613 g=-0.012\n",
            ">9, 135/225, c1=-1.497, c2=0.727 g=-0.012\n",
            ">9, 136/225, c1=0.016, c2=0.720 g=-0.012\n",
            ">9, 137/225, c1=-0.862, c2=-0.971 g=-0.012\n",
            ">9, 138/225, c1=-1.222, c2=-0.538 g=-0.012\n",
            ">9, 139/225, c1=-0.282, c2=1.149 g=-0.012\n",
            ">9, 140/225, c1=1.934, c2=2.377 g=-0.012\n",
            ">9, 141/225, c1=-0.334, c2=1.247 g=-0.012\n",
            ">9, 142/225, c1=-1.328, c2=-0.239 g=-0.012\n",
            ">9, 143/225, c1=-1.322, c2=-1.218 g=-0.012\n",
            ">9, 144/225, c1=0.626, c2=-2.280 g=-0.012\n",
            ">9, 145/225, c1=-0.991, c2=-1.126 g=-0.012\n",
            ">9, 146/225, c1=-1.375, c2=4.374 g=-0.012\n",
            ">9, 147/225, c1=-0.920, c2=-0.567 g=-0.012\n",
            ">9, 148/225, c1=-0.036, c2=-1.653 g=-0.012\n",
            ">9, 149/225, c1=-1.003, c2=-0.367 g=-0.012\n",
            ">9, 150/225, c1=2.406, c2=0.545 g=-0.012\n",
            ">9, 151/225, c1=1.271, c2=-0.282 g=-0.012\n",
            ">9, 152/225, c1=-0.072, c2=-0.183 g=-0.012\n",
            ">9, 153/225, c1=-0.411, c2=1.321 g=-0.012\n",
            ">9, 154/225, c1=1.311, c2=-0.764 g=-0.012\n",
            ">9, 155/225, c1=-0.659, c2=-1.580 g=-0.012\n",
            ">9, 156/225, c1=-1.435, c2=1.023 g=-0.012\n",
            ">9, 157/225, c1=-1.784, c2=-0.978 g=-0.012\n",
            ">9, 158/225, c1=-1.123, c2=2.746 g=-0.012\n",
            ">9, 159/225, c1=0.148, c2=-0.322 g=-0.012\n",
            ">9, 160/225, c1=-1.006, c2=0.588 g=-0.012\n",
            ">9, 161/225, c1=-1.167, c2=-1.460 g=-0.012\n",
            ">9, 162/225, c1=0.565, c2=0.486 g=-0.012\n",
            ">9, 163/225, c1=-1.223, c2=0.080 g=-0.012\n",
            ">9, 164/225, c1=0.148, c2=1.655 g=-0.012\n",
            ">9, 165/225, c1=2.285, c2=-1.029 g=-0.012\n",
            ">9, 166/225, c1=-1.553, c2=1.888 g=-0.012\n",
            ">9, 167/225, c1=-0.718, c2=-2.760 g=-0.012\n",
            ">9, 168/225, c1=-0.080, c2=1.827 g=-0.012\n",
            ">9, 169/225, c1=0.578, c2=0.105 g=-0.012\n",
            ">9, 170/225, c1=1.191, c2=1.159 g=-0.012\n",
            ">9, 171/225, c1=-0.581, c2=3.077 g=-0.012\n",
            ">9, 172/225, c1=-0.661, c2=2.565 g=-0.012\n",
            ">9, 173/225, c1=-1.631, c2=-0.612 g=-0.012\n",
            ">9, 174/225, c1=-0.754, c2=-0.895 g=-0.012\n",
            ">9, 175/225, c1=-1.040, c2=0.151 g=-0.012\n",
            ">9, 176/225, c1=-0.359, c2=-0.802 g=-0.012\n",
            ">9, 177/225, c1=-0.226, c2=-1.809 g=-0.012\n",
            ">9, 178/225, c1=-1.168, c2=2.630 g=-0.012\n",
            ">9, 179/225, c1=1.045, c2=-1.369 g=-0.012\n",
            ">9, 180/225, c1=-0.837, c2=2.154 g=-0.012\n",
            ">9, 181/225, c1=-0.347, c2=0.453 g=-0.012\n",
            ">9, 182/225, c1=-2.187, c2=0.770 g=-0.012\n",
            ">9, 183/225, c1=-1.654, c2=-1.811 g=-0.012\n",
            ">9, 184/225, c1=0.744, c2=-0.499 g=-0.012\n",
            ">9, 185/225, c1=-1.415, c2=-1.661 g=-0.012\n",
            ">9, 186/225, c1=-1.300, c2=2.540 g=-0.012\n",
            ">9, 187/225, c1=2.761, c2=-0.471 g=-0.012\n",
            ">9, 188/225, c1=-1.343, c2=0.157 g=-0.012\n",
            ">9, 189/225, c1=-2.324, c2=0.646 g=-0.012\n",
            ">9, 190/225, c1=-1.023, c2=3.233 g=-0.012\n",
            ">9, 191/225, c1=-0.292, c2=1.086 g=-0.012\n",
            ">9, 192/225, c1=-1.208, c2=1.597 g=-0.012\n",
            ">9, 193/225, c1=-1.150, c2=-0.136 g=-0.012\n",
            ">9, 194/225, c1=-0.319, c2=0.656 g=-0.012\n",
            ">9, 195/225, c1=-0.466, c2=-1.760 g=-0.012\n",
            ">9, 196/225, c1=-0.454, c2=-0.664 g=-0.012\n",
            ">9, 197/225, c1=-1.006, c2=-1.103 g=-0.012\n",
            ">9, 198/225, c1=-0.097, c2=0.427 g=-0.012\n",
            ">9, 199/225, c1=0.586, c2=-0.750 g=-0.012\n",
            ">9, 200/225, c1=2.764, c2=-0.396 g=-0.012\n",
            ">9, 201/225, c1=1.682, c2=-2.461 g=-0.012\n",
            ">9, 202/225, c1=-1.296, c2=-0.806 g=-0.012\n",
            ">9, 203/225, c1=0.143, c2=1.602 g=-0.012\n",
            ">9, 204/225, c1=1.141, c2=-0.821 g=-0.012\n",
            ">9, 205/225, c1=-0.300, c2=0.580 g=-0.012\n",
            ">9, 206/225, c1=-1.608, c2=-1.963 g=-0.012\n",
            ">9, 207/225, c1=-1.725, c2=-0.951 g=-0.012\n",
            ">9, 208/225, c1=-1.257, c2=-0.691 g=-0.012\n",
            ">9, 209/225, c1=-0.964, c2=-0.267 g=-0.012\n",
            ">9, 210/225, c1=-1.477, c2=-0.424 g=-0.012\n",
            ">9, 211/225, c1=1.286, c2=-0.935 g=-0.012\n",
            ">9, 212/225, c1=0.060, c2=-0.815 g=-0.012\n",
            ">9, 213/225, c1=-0.519, c2=-2.059 g=-0.012\n",
            ">9, 214/225, c1=-1.003, c2=-0.280 g=-0.012\n",
            ">9, 215/225, c1=3.042, c2=0.376 g=-0.012\n",
            ">9, 216/225, c1=-1.680, c2=-1.140 g=-0.012\n",
            ">9, 217/225, c1=0.264, c2=-0.696 g=-0.012\n",
            ">9, 218/225, c1=-0.649, c2=-0.334 g=-0.012\n",
            ">9, 219/225, c1=1.008, c2=-1.535 g=-0.012\n",
            ">9, 220/225, c1=-1.115, c2=0.516 g=-0.012\n",
            ">9, 221/225, c1=-0.348, c2=-1.529 g=-0.012\n",
            ">9, 222/225, c1=0.272, c2=1.098 g=-0.012\n",
            ">9, 223/225, c1=-0.794, c2=1.315 g=-0.012\n",
            ">9, 224/225, c1=-0.083, c2=0.827 g=-0.012\n",
            ">9, 225/225, c1=0.671, c2=-1.687 g=-0.012\n",
            ">10, 1/225, c1=-1.548, c2=1.922 g=-0.012\n",
            ">10, 2/225, c1=-1.296, c2=-1.584 g=-0.012\n",
            ">10, 3/225, c1=-0.559, c2=3.212 g=-0.012\n",
            ">10, 4/225, c1=-0.296, c2=2.144 g=-0.012\n",
            ">10, 5/225, c1=2.650, c2=-1.335 g=-0.012\n",
            ">10, 6/225, c1=-0.477, c2=1.519 g=-0.012\n",
            ">10, 7/225, c1=-1.706, c2=3.458 g=-0.012\n",
            ">10, 8/225, c1=-1.357, c2=2.428 g=-0.012\n",
            ">10, 9/225, c1=-1.028, c2=-1.984 g=-0.012\n",
            ">10, 10/225, c1=1.613, c2=0.638 g=-0.012\n",
            ">10, 11/225, c1=-0.239, c2=-0.023 g=-0.012\n",
            ">10, 12/225, c1=-1.697, c2=0.203 g=-0.012\n",
            ">10, 13/225, c1=-0.328, c2=1.684 g=-0.012\n",
            ">10, 14/225, c1=-0.563, c2=1.251 g=-0.012\n",
            ">10, 15/225, c1=-1.612, c2=1.860 g=-0.012\n",
            ">10, 16/225, c1=0.235, c2=-0.985 g=-0.012\n",
            ">10, 17/225, c1=-1.310, c2=0.530 g=-0.012\n",
            ">10, 18/225, c1=0.984, c2=1.565 g=-0.012\n",
            ">10, 19/225, c1=-1.590, c2=-3.218 g=-0.012\n",
            ">10, 20/225, c1=-0.054, c2=1.963 g=-0.012\n",
            ">10, 21/225, c1=0.278, c2=2.863 g=-0.012\n",
            ">10, 22/225, c1=0.646, c2=-1.909 g=-0.012\n",
            ">10, 23/225, c1=1.232, c2=-1.116 g=-0.012\n",
            ">10, 24/225, c1=1.085, c2=2.272 g=-0.012\n",
            ">10, 25/225, c1=-1.483, c2=0.276 g=-0.012\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-4dc2ba0c028a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-794aec125b49>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(initial_epoch, latent_dim)\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   train(g_model, c_model, gan_model, dataset, latent_dim, n_epochs=1200,\n\u001b[0;32m---> 56\u001b[0;31m         csv_path=csv_path,initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Elapsed time: {time.time() - start} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-29311409c0e3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(g_model, c_model, gan_model, dataset, latent_dim, csv_path, n_epochs, n_batch, n_critic, initial_epoch)\u001b[0m\n\u001b[1;32m    257\u001b[0m                         \u001b[0mc1_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_loss1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                         \u001b[0;31m# generate 'fake' examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                         \u001b[0mX_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_fake_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalf_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m                         \u001b[0;31m# update critic model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                         \u001b[0mc_loss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-29311409c0e3>\u001b[0m in \u001b[0;36mgenerate_fake_samples\u001b[0;34m(generator, latent_dim, n_samples, label_noising, p_flip)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mx_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_latent_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# predict outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;31m# create class labels with 1.0 for 'fake'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3798\u001b[0m     return nest.pack_sequence_as(\n\u001b[1;32m   3799\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3800\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3801\u001b[0m         expand_composites=True)\n\u001b[1;32m   3802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3798\u001b[0m     return nest.pack_sequence_as(\n\u001b[1;32m   3799\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3800\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3801\u001b[0m         expand_composites=True)\n\u001b[1;32m   3802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ml50lven5Dp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_hist = pd.read_csv(\"loss.csv\", index_col=None)\n",
        "list_hist = df_hist.values.tolist()\n",
        "list_hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oahRtfoqiIJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "last_step = max([int(re.findall('[0-9]+',s.split('.')[0])[0]) for s in os.listdir('model_checkpoints')])\n",
        "last_epoch = max([int(re.findall('[0-9]+',s.split('.')[0])[0]) for s in os.listdir('model_checkpoints')])\n",
        "\n",
        "model_paths = [\"generator_model_%03d.h5\",\"critic_model_weights_%03d.h5\",\n",
        "              \"GAN_model_weights_%03d.h5\",\"GAN_optimizer_weights_%03d.npy\"]\n",
        "model_filenames = [os.path.join(\"model_checkpoints\", p % (last_epoch))  for p in model_paths]  \n",
        "print(model_filenames, last_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhwsiXPRGJnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g_model = load_model(os.path.join('model_checkpoints','generator_model_015.h5'))\n",
        "latent_dim = 250\n",
        "n_samples = 100\n",
        "X, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "# scale from [-1,1] to [0,1]\n",
        "X = (X + 1) / 2.0\n",
        "# plot images\n",
        "print(X.shape)\n",
        "for i in range(7 * 7):\n",
        "  # define subplot\n",
        "  pyplot.subplot(7, 7, 1 + i)\n",
        "  # turn off axis\n",
        "  pyplot.axis('off')\n",
        "  # plot raw pixel data\n",
        "  pyplot.imshow(X[i,:,:,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mngk45GKIrwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "df = pd.read_csv('loss.csv', index_col=None)\n",
        "# print(df[df.columns].loc[df['steps']<=225])\n",
        "# df.insert(0, 'epoch', '')\n",
        "# df['epoch'] = df['steps'].map(lambda x: math.ceil(x/225))\n",
        "# df['steps'] = df['steps'].map(lambda x: \"%d/%d\" % ((x%225) if (x%225)!=0 else 225, 225))\n",
        "# df.to_csv('loss.csv', index=False)\n",
        "# df = df.loc[df['epoch'] <= 12 ]\n",
        "# df = df.drop(df.index[-1])\n",
        "# df.to_csv('loss.csv', index=False)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHoZi3CacLRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# os.chdir('figure_plots')\n",
        "# print(os.getcwd())\n",
        "# for plot in os.listdir('.'):\n",
        "#   steps = int(re.findall('[0-9]+',plot)[0])\n",
        "#   new_fname = \"generated_plot_%03d.png\" % (steps/225)\n",
        "#   os.rename(plot, new_fname)\n",
        "#   print(plot,new_fname)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr4wL-T9eUOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_models = filter(lambda x: x,[re.findall('[0-9]+',s.split('.')[0]) for s in os.listdir('model_checkpoints')])\n",
        "last_epoch = max([int(g[0]) for g in generator_models])\n",
        "last_epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7WtnF6S-kAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a = [a*225 for a in range(1,10)]\n",
        "# for i in a:\n",
        "#   if i % (225*2) == 0:\n",
        "#     print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQJ8qfjQIKN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}