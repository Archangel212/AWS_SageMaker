{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN_BATIK_Dataset_64by64_UNet_architecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1X8eQnyh1eab4Ct9e64PRGohUyoCqFjPw",
      "authorship_tag": "ABX9TyMOnEqZwlgn7l9mPKGPOSlc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Archangel212/GAN_jupyter_notebooks/blob/master/WGAN_BATIK_Dataset_64by64_UNet_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMTZp16sdxpO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "50e0e776-fa33-4eac-f8c0-8f093909987f"
      },
      "source": [
        "# example of a wgan for generating handwritten digits\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive')\n",
        "\n",
        "from numpy import expand_dims\n",
        "from numpy import mean\n",
        "from numpy import ones\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.datasets.mnist import load_data\n",
        "from keras import backend\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.models import Sequential,save_model,load_model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.constraints import Constraint\n",
        "from keras.utils import plot_model\n",
        "from matplotlib import pyplot\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import h5py\n",
        "import pandas as pd\n",
        "from utils.save_model_summary import save_model_summary\n",
        "from utils.trim_csv import trim_csv\n",
        "import math\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/WGAN/BATIK_Dataset_64by64_UNet_architecture\")\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SpVfhzLfwPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clip model weights to a given hypercube\n",
        "class ClipConstraint(Constraint):\n",
        "\t# set clip value when initialized\n",
        "\tdef __init__(self, clip_value):\n",
        "\t\tself.clip_value = clip_value\n",
        "\n",
        "\t# clip model weights to hypercube\n",
        "\tdef __call__(self, weights):\n",
        "\t\treturn backend.clip(weights, -self.clip_value, self.clip_value)\n",
        "\n",
        "\t# get the config\n",
        "\tdef get_config(self):\n",
        "\t\treturn {'clip_value': self.clip_value}\n",
        "\n",
        "# calculate wasserstein loss\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "\treturn backend.mean(y_true * y_pred)\n",
        "\n",
        "# define the standalone critic model\n",
        "def define_critic(in_shape=(64,64,3)):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# weight constraint\n",
        "\tconst = ClipConstraint(0.01)\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\n",
        "\t# input layer\n",
        "\tmodel.add(Conv2D(64, (3,3), padding='same', input_shape=in_shape, kernel_initializer=init, kernel_constraint=const))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# downsample to 32x32\n",
        "\tmodel.add(Conv2D(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init, kernel_constraint=const))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# downsample to 16x16\n",
        "\tmodel.add(Conv2D(128, (3,3), strides=(2,2), padding='same',kernel_initializer=init, kernel_constraint=const))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# downsample to 8x8\n",
        "\tmodel.add(Conv2D(256, (3,3), strides=(2,2), padding='same',kernel_initializer=init, kernel_constraint=const))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# downsample to 4x4\n",
        "\tmodel.add(Conv2D(512, (3,3), strides=(2,2), padding='same',kernel_initializer=init, kernel_constraint=const))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# downsample to 2x2\n",
        "\tmodel.add(Conv2D(512, (3,3), strides=(2,2), padding='same',kernel_initializer=init, kernel_constraint=const))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# downsample to 1x1\n",
        "\tmodel.add(Conv2D(512, (3,3), strides=(2,2), padding='same',kernel_initializer=init, kernel_constraint=const))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# scoring, linear activation\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(1))\n",
        "\t# compile model\n",
        "\topt = RMSprop(lr=0.00005)\n",
        "\tmodel.compile(loss=wasserstein_loss, optimizer=opt)\n",
        "\treturn model\n",
        "\n",
        "# define the standalone generator model\n",
        "def define_generator(latent_dim):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\t# foundation for 2x2 image\n",
        "\tn_nodes = 512 * 2 * 2\n",
        "\tmodel.add(Dense(n_nodes, kernel_initializer=init, input_dim=latent_dim))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\tmodel.add(Reshape((2, 2, 512)))\n",
        "\t# upsample to 4x4\n",
        "\tmodel.add(Conv2DTranspose(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# upsample to 8x8\n",
        "\tmodel.add(Conv2DTranspose(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# upsample to 16x16\n",
        "\tmodel.add(Conv2DTranspose(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# upsample to 32x32\n",
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same',kernel_initializer=init))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# upsample to 64x64\n",
        "\tmodel.add(Conv2DTranspose(64, (4,4), strides=(2,2), padding='same',kernel_initializer=init))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(BatchNormalization(axis=-1))\n",
        "\t# output 64x64x3\n",
        "\tmodel.add(Conv2D(3, (3,3), activation='tanh', padding='same', kernel_initializer=init))\n",
        "\treturn model\n",
        "\n",
        "# define the combined generator and critic model, for updating the generator\n",
        "def define_gan(generator, critic):\n",
        "\t# make weights in the critic not trainable\n",
        "\tcritic.trainable = False\n",
        "\t# connect them\n",
        "\tmodel = Sequential()\n",
        "\t# add generator\n",
        "\tmodel.add(generator)\n",
        "\t# add the critic\n",
        "\tmodel.add(critic)\n",
        "\t# compile model\n",
        "\topt = RMSprop(lr=0.00005)\n",
        "\tmodel.compile(loss=wasserstein_loss, optimizer=opt)\n",
        "\treturn model\n",
        "\n",
        "\n",
        "def load_dataset(ds_path):\n",
        "  with h5py.File(ds_path,\"r\") as f:\n",
        "    dataset = f[\"Batik\"]\n",
        "    dataset = np.copy(dataset)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "\t\n",
        "# load images\n",
        "def load_real_samples():\n",
        "\t# load cifar10 dataset\n",
        "\tds_path = \"/content/drive/My Drive/Batik_Datasets/pinterest_version/batik_dataset_64by64.hdf5\"\n",
        "\ttrainX = load_dataset(ds_path)\n",
        "\t# convert from unsigned ints to floats\n",
        "\tX = trainX.astype('float32')\n",
        "\t# scale from [0,255] to [-1,1]\n",
        "\tX = (X - 127.5) / 127.5\n",
        "\treturn X\n",
        "\n",
        "def noisy_labels(y, p_flip):\n",
        "\t# ix = np.random.choice(y.shape[0], size=int(y.shape[0]*p_flip), replace=False)\n",
        "\t# y[ix] = 1 - y[ix]\n",
        "\tn_select = int(p_flip * y.shape[0])\n",
        "\t# choose labels to flip\n",
        "\tflip_ix = np.random.choice([i for i in range(y.shape[0])], size=n_select,replace=False)\n",
        "\t# invert the labels in place\n",
        "\ty[flip_ix] =  -1 *  y[flip_ix]\n",
        "\treturn y\n",
        "\n",
        "# select real samples\n",
        "def generate_real_samples(dataset, n_samples, label_noising=True, p_flip=0.05):\n",
        "\t# choose random instances\n",
        "\tix = randint(0, dataset.shape[0], n_samples)\n",
        "\t# select images\n",
        "\tX = dataset[ix]\n",
        "\t# generate class labels, -1 for 'real'\n",
        "\ty = -ones((n_samples, 1))\n",
        " \n",
        "\tif label_noising:\n",
        "\t\ty = noisy_labels(y, p_flip) \n",
        "\t\n",
        "\treturn X, y\n",
        "\n",
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn x_input\n",
        "\n",
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(generator, latent_dim, n_samples, label_noising=True, p_flip=0.05):\n",
        "\t# generate points in latent space\n",
        "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
        "\t# predict outputs\n",
        "\tX = generator.predict(x_input)\n",
        "\t# create class labels with 1.0 for 'fake'\n",
        "\ty = ones((n_samples, 1))\n",
        " \n",
        "\tif label_noising:\n",
        "\t\ty = noisy_labels(y, p_flip)\n",
        "\t\n",
        "\treturn X, y\n",
        "\n",
        "# generate samples and save as a plot and save the model\n",
        "def summarize_performance(epoch,c_model, g_model, gan_model, latent_dim,n_samples=100):\n",
        "\n",
        "\t# prepare fake examples\n",
        "\tX, _ = generate_fake_samples(g_model, latent_dim, n_samples, label_noising=False)\n",
        "\t# scale from [-1,1] to [0,1]\n",
        "\tX = (X + 1) / 2.0\n",
        "\t# plot images\n",
        "\tfor i in range(7 * 7):\n",
        "\t\t# define subplot\n",
        "\t\tpyplot.subplot(7, 7, 1 + i)\n",
        "\t\t# turn off axis\n",
        "\t\tpyplot.axis('off')\n",
        "\t\t# plot raw pixel data\n",
        "\t\tpyplot.imshow(X[i])\n",
        "\t# save plot to file\n",
        "\tfigure_plot = os.path.join('figure_plots','generated_plot_%03d.png' % (epoch))\n",
        "\tpyplot.savefig(figure_plot)\n",
        "\tpyplot.close()\n",
        " \n",
        "\t#save critic model\n",
        "\tcritic_model_weights_fname = os.path.join('model_checkpoints','critic_model_weights.h5')\n",
        "\tc_model.save_weights(critic_model_weights_fname)\n",
        "\n",
        "\tgenerator_fname = os.path.join('model_checkpoints', 'generator_model_%03d.h5' % (epoch))\n",
        "\tsave_model(g_model, generator_fname)\n",
        "\n",
        "\tgan_model_weights_fname = os.path.join('model_checkpoints', 'gan_model_weights.h5')\n",
        "\tgan_model.save_weights(gan_model_weights_fname)\n",
        "\n",
        "\tgan_optimizer_weights_fname = os.path.join('model_checkpoints', 'gan_optimizer_weights.npy')\n",
        "\tgan_optimizer_weights = gan_model.optimizer.get_weights()\n",
        "\tnp.save(gan_optimizer_weights_fname, gan_optimizer_weights, allow_pickle=True)\n",
        "\n",
        "\tprint(f'Model_epoch_{epoch} saved')\n",
        "\n",
        "# create a line plot of loss for the gan and save to file\n",
        "def plot_history(df_hist):\n",
        "\t# plot history\n",
        "\tpyplot.plot(df_hist[\"critic_loss_real\"], label='critic_real')\n",
        "\tpyplot.plot(df_hist[\"critic_loss_fake\"], label='critic_fake')\n",
        "\tpyplot.plot(df_hist[\"gan_loss\"], label='gen')\n",
        "\tpyplot.xlabel(\"number of iterations\")\n",
        "\tpyplot.ylabel(\"loss\")\n",
        "\tpyplot.legend()\n",
        "\tpyplot.savefig('plot_line_plot_loss.png')\n",
        "\tpyplot.close()\n",
        "\n",
        "\n",
        "# train the generator and critic\n",
        "def train_wgan(g_model, c_model, gan_model, dataset, latent_dim, csv_path=None, n_epochs=10, n_batch=64, n_critic=5, initial_epoch=0):\n",
        "\t# calculate the number of batches per training epoch\n",
        "\tbat_per_epo = int(dataset.shape[0] / n_batch)\n",
        "\tinitial_step = initial_epoch*bat_per_epo\n",
        "\t\n",
        "\tprint(dataset.shape,\"bat_per_epo: \", bat_per_epo)\n",
        "\t# calculate the number of training iterations\n",
        "\tn_steps = bat_per_epo * n_epochs\n",
        "\t# calculate the size of half a batch of samples\n",
        "\thalf_batch = int(n_batch / 2)\n",
        "\t# lists for keeping track of loss\n",
        "\t# c1_hist, c2_hist, g_hist = list(), list(), list()\n",
        "\tdf_hist = pd.read_csv(csv_path,index_col=None)\n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(initial_step, n_steps):\n",
        "\t\t# update the critic more than the generator\n",
        "\t\tepoch = math.ceil((i+1)/bat_per_epo)\n",
        "\t\tsteps = \"%d/%d\" % ((i+1)%bat_per_epo if (i+1)%bat_per_epo != 0 else bat_per_epo ,bat_per_epo)  \n",
        "\t\n",
        "\t\tc1_tmp, c2_tmp = list(), list()\n",
        "\t\tfor _ in range(n_critic):\n",
        "\t\t\t# get randomly selected 'real' samples\n",
        "\t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "\t\t\t# update critic model weights\n",
        "\t\t\tc_loss1 = c_model.train_on_batch(X_real, y_real)\n",
        "\t\t\tc1_tmp.append(c_loss1)\n",
        "\t\t\t# generate 'fake' examples\n",
        "\t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "\t\t\t# update critic model weights\n",
        "\t\t\tc_loss2 = c_model.train_on_batch(X_fake, y_fake)\n",
        "\t\t\tc2_tmp.append(c_loss2)\n",
        "\t\t# prepare points in latent space as input for the generator\n",
        "\t\tX_gan = generate_latent_points(latent_dim, n_batch)\n",
        "\t\t# create inverted labels for the fake samples\n",
        "\t\ty_gan = -ones((n_batch, 1))\n",
        "\t\t# update the generator via the critic's error\n",
        "\t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "\t\t# g_hist.append(g_loss)\n",
        "\t\tdf_hist = df_hist.append({\n",
        "\t\t\t\t\"epoch\"\t: epoch,\n",
        "\t\t\t\t\"steps\" : steps,\n",
        "\t\t\t\t\"critic_loss_real\" : mean(c1_tmp),\n",
        "\t\t\t\t\"critic_loss_fake\" : mean(c2_tmp),\n",
        "\t\t\t\t\"gan_loss\" : g_loss\n",
        "\t\t}, ignore_index=True)\n",
        "\n",
        "\n",
        "\t\t# summarize loss on this batch\n",
        "\t\tprint('>%d, %s, c1=%.3f, c2=%.3f g=%.3f' % (epoch, steps, df_hist[\"critic_loss_real\"].iloc[-1], \n",
        "\t\t                                        df_hist[\"critic_loss_fake\"].iloc[-1], df_hist[\"gan_loss\"].iloc[-1]))\n",
        "\t\t# evaluate the model performance every '2 epoch'\n",
        "\t\tif (i+1) % (bat_per_epo*2) == 0 or (i+1) == bat_per_epo:\n",
        "\t\t\tsummarize_performance(epoch, c_model, g_model, gan_model, latent_dim)\n",
        "\t\t\tdf_hist.to_csv(csv_path, mode=\"w\", index=False)\n",
        "\t\t\tplot_history(df_hist)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdeyWY1jhTii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train(resume_training=False, initial_epoch=0, latent_dim=250):\n",
        "  csv_path = \"loss.csv\"\n",
        "  if not os.path.exists(csv_path): \n",
        "    with open(csv_path,'a+') as f:\n",
        "      f.write('epoch,steps,critic_loss_real,critic_loss_fake,gan_loss')\n",
        "  os.makedirs('figure_plots', exist_ok=True)\n",
        "  os.makedirs('model_checkpoints', exist_ok=True)\n",
        "  os.makedirs('model_summaries', exist_ok=True)\n",
        "  if resume_training == False:\n",
        "    # create the critic\n",
        "    c_model = define_critic()\n",
        "    # create the generator\n",
        "    g_model = define_generator(latent_dim)\n",
        "    # create the gan\n",
        "    gan_model = define_gan(g_model, c_model)\n",
        "\n",
        "  else:\n",
        "    generator_models = filter(lambda x: x,[re.findall('[0-9]+',s.split('.')[0]) for s in os.listdir('model_checkpoints')])\n",
        "    last_epoch = max([int(g[0]) for g in generator_models])\n",
        "\n",
        "    trim_csv(csv_path, last_epoch)\n",
        "    \n",
        "    model_paths = [\"critic_model_weights.h5\",\"gan_model_weights.h5\",\"gan_optimizer_weights.npy\"]\n",
        "    model_filenames = [os.path.join(\"model_checkpoints\", p)  for p in model_paths] \n",
        "\n",
        "    g_model = load_model(os.path.join(\"model_checkpoints\",\"generator_model_%03d.h5\" % (last_epoch)))\n",
        "    c_model = define_critic()\n",
        "    c_model.load_weights(model_filenames[0])\n",
        "\n",
        "    gan_model = define_gan(g_model, c_model)\n",
        "    gan_model.load_weights(model_filenames[1])\n",
        "    gan_model._make_train_function()\n",
        "\n",
        "    gan_model_optimizer_weights = np.load(model_filenames[2], allow_pickle=True).tolist()\n",
        "    gan_model.optimizer.set_weights(gan_model_optimizer_weights)\n",
        "\n",
        "    initial_epoch = last_epoch    \n",
        "    print(f\"Last trained model at epoch : {last_epoch}\")\n",
        "    print(\"Resume Training ...\")\n",
        "\n",
        "  # load image data\n",
        "  dataset = load_real_samples()\n",
        "  \n",
        "\n",
        "  models = [g_model,c_model,gan_model]\n",
        "  filenames = ['Generator_model.png','Critic_model.png','GAN_model.png']\n",
        "  for (model,fn) in zip(models,filenames):\n",
        "    plot_model(model, to_file=fn, show_shapes=True, show_layer_names=True)\n",
        "    save_model_summary(model, os.path.join('model_summaries', fn))\n",
        "\n",
        "  # train model\n",
        "  train_wgan(g_model, c_model, gan_model, dataset, latent_dim,\n",
        "             csv_path=csv_path, n_epochs=400, initial_epoch=initial_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdxnx6T8gLkq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ac99e312-3770-4120-a032-fc7f7d11f0e1"
      },
      "source": [
        "train(resume_training=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "csv trimed at epoch > 66\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Last trained model at epoch : 66\n",
            "Resume Training ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(14400, 64, 64, 3) bat_per_epo:  225\n",
            ">67, 1/225, c1=-4.986, c2=-0.419 g=-0.020\n",
            ">67, 2/225, c1=0.972, c2=0.207 g=-0.021\n",
            ">67, 3/225, c1=-2.872, c2=-7.779 g=-0.021\n",
            ">67, 4/225, c1=3.348, c2=3.803 g=-0.021\n",
            ">67, 5/225, c1=-6.560, c2=4.942 g=-0.021\n",
            ">67, 6/225, c1=3.950, c2=4.962 g=-0.021\n",
            ">67, 7/225, c1=-1.995, c2=6.472 g=-0.021\n",
            ">67, 8/225, c1=-0.045, c2=-1.346 g=-0.021\n",
            ">67, 9/225, c1=4.562, c2=0.732 g=-0.021\n",
            ">67, 10/225, c1=-3.537, c2=1.174 g=-0.021\n",
            ">67, 11/225, c1=-9.031, c2=10.489 g=-0.021\n",
            ">67, 12/225, c1=1.516, c2=7.419 g=-0.021\n",
            ">67, 13/225, c1=-1.260, c2=3.763 g=-0.021\n",
            ">67, 14/225, c1=0.533, c2=-0.217 g=-0.021\n",
            ">67, 15/225, c1=20.051, c2=3.564 g=-0.021\n",
            ">67, 16/225, c1=-10.160, c2=4.006 g=-0.021\n",
            ">67, 17/225, c1=-0.226, c2=-7.056 g=-0.021\n",
            ">67, 18/225, c1=0.090, c2=0.913 g=-0.021\n",
            ">67, 19/225, c1=-0.143, c2=-19.443 g=-0.021\n",
            ">67, 20/225, c1=-7.900, c2=3.272 g=-0.021\n",
            ">67, 21/225, c1=-6.040, c2=-3.069 g=-0.021\n",
            ">67, 22/225, c1=11.683, c2=11.693 g=-0.021\n",
            ">67, 23/225, c1=0.561, c2=-6.794 g=-0.021\n",
            ">67, 24/225, c1=-8.782, c2=1.768 g=-0.021\n",
            ">67, 25/225, c1=2.041, c2=-2.134 g=-0.021\n",
            ">67, 26/225, c1=-7.016, c2=-6.299 g=-0.021\n",
            ">67, 27/225, c1=5.585, c2=-4.309 g=-0.021\n",
            ">67, 28/225, c1=3.583, c2=-3.407 g=-0.021\n",
            ">67, 29/225, c1=14.896, c2=2.766 g=-0.021\n",
            ">67, 30/225, c1=4.693, c2=-0.652 g=-0.021\n",
            ">67, 31/225, c1=10.888, c2=-6.533 g=-0.021\n",
            ">67, 32/225, c1=12.537, c2=7.710 g=-0.021\n",
            ">67, 33/225, c1=-1.290, c2=8.469 g=-0.021\n",
            ">67, 34/225, c1=7.590, c2=9.420 g=-0.021\n",
            ">67, 35/225, c1=-7.409, c2=2.422 g=-0.021\n",
            ">67, 36/225, c1=0.171, c2=2.338 g=-0.021\n",
            ">67, 37/225, c1=0.807, c2=-8.790 g=-0.021\n",
            ">67, 38/225, c1=-1.468, c2=0.542 g=-0.021\n",
            ">67, 39/225, c1=5.579, c2=3.726 g=-0.021\n",
            ">67, 40/225, c1=-4.486, c2=-3.131 g=-0.021\n",
            ">67, 41/225, c1=-1.916, c2=-9.382 g=-0.021\n",
            ">67, 42/225, c1=-2.281, c2=9.244 g=-0.021\n",
            ">67, 43/225, c1=-10.225, c2=-3.263 g=-0.021\n",
            ">67, 44/225, c1=-1.954, c2=3.535 g=-0.021\n",
            ">67, 45/225, c1=0.299, c2=-1.921 g=-0.021\n",
            ">67, 46/225, c1=3.766, c2=-4.803 g=-0.021\n",
            ">67, 47/225, c1=7.663, c2=8.680 g=-0.021\n",
            ">67, 48/225, c1=-3.566, c2=-8.231 g=-0.021\n",
            ">67, 49/225, c1=-10.111, c2=3.730 g=-0.021\n",
            ">67, 50/225, c1=2.867, c2=2.602 g=-0.021\n",
            ">67, 51/225, c1=-7.298, c2=3.545 g=-0.021\n",
            ">67, 52/225, c1=-4.149, c2=0.507 g=-0.021\n",
            ">67, 53/225, c1=-6.616, c2=-10.427 g=-0.021\n",
            ">67, 54/225, c1=-1.368, c2=6.169 g=-0.021\n",
            ">67, 55/225, c1=6.292, c2=-14.096 g=-0.021\n",
            ">67, 56/225, c1=14.110, c2=-1.584 g=-0.021\n",
            ">67, 57/225, c1=-13.821, c2=6.803 g=-0.021\n",
            ">67, 58/225, c1=2.235, c2=10.784 g=-0.021\n",
            ">67, 59/225, c1=-9.933, c2=1.870 g=-0.021\n",
            ">67, 60/225, c1=-2.628, c2=-2.718 g=-0.021\n",
            ">67, 61/225, c1=-5.081, c2=4.399 g=-0.021\n",
            ">67, 62/225, c1=1.840, c2=-1.522 g=-0.021\n",
            ">67, 63/225, c1=-3.275, c2=2.929 g=-0.021\n",
            ">67, 64/225, c1=2.015, c2=0.402 g=-0.021\n",
            ">67, 65/225, c1=-7.088, c2=2.171 g=-0.021\n",
            ">67, 66/225, c1=1.149, c2=-8.593 g=-0.021\n",
            ">67, 67/225, c1=10.991, c2=2.261 g=-0.021\n",
            ">67, 68/225, c1=-8.563, c2=2.407 g=-0.021\n",
            ">67, 69/225, c1=-8.584, c2=-7.982 g=-0.021\n",
            ">67, 70/225, c1=-3.433, c2=16.758 g=-0.021\n",
            ">67, 71/225, c1=0.605, c2=5.026 g=-0.021\n",
            ">67, 72/225, c1=1.889, c2=-0.479 g=-0.021\n",
            ">67, 73/225, c1=-5.971, c2=-2.316 g=-0.021\n",
            ">67, 74/225, c1=8.404, c2=5.816 g=-0.021\n",
            ">67, 75/225, c1=-13.417, c2=-3.109 g=-0.021\n",
            ">67, 76/225, c1=11.152, c2=-10.311 g=-0.021\n",
            ">67, 77/225, c1=-9.066, c2=2.302 g=-0.021\n",
            ">67, 78/225, c1=2.038, c2=-3.763 g=-0.021\n",
            ">67, 79/225, c1=-7.215, c2=-8.566 g=-0.021\n",
            ">67, 80/225, c1=3.717, c2=-8.330 g=-0.021\n",
            ">67, 81/225, c1=-2.249, c2=5.456 g=-0.021\n",
            ">67, 82/225, c1=0.981, c2=8.508 g=-0.021\n",
            ">67, 83/225, c1=7.454, c2=-4.978 g=-0.021\n",
            ">67, 84/225, c1=-3.379, c2=2.545 g=-0.021\n",
            ">67, 85/225, c1=-3.182, c2=4.016 g=-0.021\n",
            ">67, 86/225, c1=4.073, c2=3.821 g=-0.021\n",
            ">67, 87/225, c1=-4.199, c2=10.703 g=-0.021\n",
            ">67, 88/225, c1=3.938, c2=-11.027 g=-0.021\n",
            ">67, 89/225, c1=-2.749, c2=-8.888 g=-0.021\n",
            ">67, 90/225, c1=-0.771, c2=0.602 g=-0.021\n",
            ">67, 91/225, c1=-3.245, c2=1.434 g=-0.021\n",
            ">67, 92/225, c1=4.276, c2=-7.115 g=-0.021\n",
            ">67, 93/225, c1=-4.031, c2=-1.895 g=-0.021\n",
            ">67, 94/225, c1=3.807, c2=2.844 g=-0.021\n",
            ">67, 95/225, c1=-0.080, c2=4.884 g=-0.021\n",
            ">67, 96/225, c1=-11.741, c2=4.546 g=-0.021\n",
            ">67, 97/225, c1=15.939, c2=2.780 g=-0.021\n",
            ">67, 98/225, c1=-1.098, c2=-9.048 g=-0.021\n",
            ">67, 99/225, c1=-3.010, c2=-0.555 g=-0.021\n",
            ">67, 100/225, c1=-0.707, c2=-5.163 g=-0.021\n",
            ">67, 101/225, c1=-8.406, c2=1.378 g=-0.021\n",
            ">67, 102/225, c1=21.448, c2=-14.461 g=-0.021\n",
            ">67, 103/225, c1=-5.472, c2=-3.458 g=-0.021\n",
            ">67, 104/225, c1=6.668, c2=-0.505 g=-0.021\n",
            ">67, 105/225, c1=-4.382, c2=0.762 g=-0.021\n",
            ">67, 106/225, c1=-4.934, c2=-9.258 g=-0.021\n",
            ">67, 107/225, c1=-3.072, c2=3.625 g=-0.021\n",
            ">67, 108/225, c1=-6.683, c2=2.077 g=-0.021\n",
            ">67, 109/225, c1=4.577, c2=10.186 g=-0.021\n",
            ">67, 110/225, c1=10.331, c2=9.594 g=-0.021\n",
            ">67, 111/225, c1=6.754, c2=1.978 g=-0.021\n",
            ">67, 112/225, c1=9.916, c2=0.754 g=-0.021\n",
            ">67, 113/225, c1=8.653, c2=-3.907 g=-0.021\n",
            ">67, 114/225, c1=-5.778, c2=2.361 g=-0.021\n",
            ">67, 115/225, c1=1.498, c2=-0.845 g=-0.021\n",
            ">67, 116/225, c1=-7.265, c2=-13.925 g=-0.021\n",
            ">67, 117/225, c1=-0.681, c2=0.609 g=-0.021\n",
            ">67, 118/225, c1=-5.800, c2=5.511 g=-0.021\n",
            ">67, 119/225, c1=1.380, c2=0.251 g=-0.021\n",
            ">67, 120/225, c1=-8.936, c2=-5.284 g=-0.021\n",
            ">67, 121/225, c1=6.151, c2=5.051 g=-0.021\n",
            ">67, 122/225, c1=4.991, c2=9.087 g=-0.021\n",
            ">67, 123/225, c1=-4.232, c2=-6.049 g=-0.021\n",
            ">67, 124/225, c1=9.106, c2=-1.871 g=-0.021\n",
            ">67, 125/225, c1=-3.616, c2=4.693 g=-0.021\n",
            ">67, 126/225, c1=-11.487, c2=-6.153 g=-0.021\n",
            ">67, 127/225, c1=-7.625, c2=-1.737 g=-0.021\n",
            ">67, 128/225, c1=-4.655, c2=9.916 g=-0.021\n",
            ">67, 129/225, c1=-1.340, c2=-9.028 g=-0.021\n",
            ">67, 130/225, c1=10.703, c2=-1.155 g=-0.021\n",
            ">67, 131/225, c1=-6.143, c2=-6.593 g=-0.021\n",
            ">67, 132/225, c1=0.005, c2=-2.075 g=-0.021\n",
            ">67, 133/225, c1=9.212, c2=-3.596 g=-0.021\n",
            ">67, 134/225, c1=-4.486, c2=7.158 g=-0.021\n",
            ">67, 135/225, c1=-6.044, c2=-6.382 g=-0.021\n",
            ">67, 136/225, c1=6.141, c2=-1.948 g=-0.021\n",
            ">67, 137/225, c1=5.035, c2=-1.499 g=-0.021\n",
            ">67, 138/225, c1=-7.807, c2=-6.803 g=-0.021\n",
            ">67, 139/225, c1=-7.434, c2=-4.621 g=-0.021\n",
            ">67, 140/225, c1=-6.012, c2=-2.910 g=-0.021\n",
            ">67, 141/225, c1=-0.330, c2=-0.737 g=-0.021\n",
            ">67, 142/225, c1=0.158, c2=0.038 g=-0.021\n",
            ">67, 143/225, c1=2.272, c2=-7.032 g=-0.021\n",
            ">67, 144/225, c1=4.048, c2=-0.498 g=-0.021\n",
            ">67, 145/225, c1=-4.212, c2=-0.582 g=-0.021\n",
            ">67, 146/225, c1=-7.219, c2=3.369 g=-0.021\n",
            ">67, 147/225, c1=-0.220, c2=5.688 g=-0.021\n",
            ">67, 148/225, c1=-6.491, c2=3.843 g=-0.021\n",
            ">67, 149/225, c1=-0.458, c2=-5.139 g=-0.021\n",
            ">67, 150/225, c1=-6.037, c2=-1.712 g=-0.021\n",
            ">67, 151/225, c1=21.656, c2=-0.162 g=-0.021\n",
            ">67, 152/225, c1=1.872, c2=-2.278 g=-0.021\n",
            ">67, 153/225, c1=-3.671, c2=8.187 g=-0.021\n",
            ">67, 154/225, c1=-7.591, c2=-3.312 g=-0.021\n",
            ">67, 155/225, c1=-4.461, c2=-0.941 g=-0.021\n",
            ">67, 156/225, c1=5.912, c2=-6.506 g=-0.021\n",
            ">67, 157/225, c1=7.120, c2=7.401 g=-0.021\n",
            ">67, 158/225, c1=3.029, c2=3.017 g=-0.021\n",
            ">67, 159/225, c1=-2.718, c2=-7.124 g=-0.021\n",
            ">67, 160/225, c1=-0.134, c2=-7.633 g=-0.021\n",
            ">67, 161/225, c1=-2.443, c2=-3.638 g=-0.021\n",
            ">67, 162/225, c1=-0.386, c2=-4.285 g=-0.021\n",
            ">67, 163/225, c1=3.913, c2=-5.050 g=-0.021\n",
            ">67, 164/225, c1=-8.411, c2=0.483 g=-0.021\n",
            ">67, 165/225, c1=-1.716, c2=1.962 g=-0.021\n",
            ">67, 166/225, c1=-2.860, c2=-7.626 g=-0.021\n",
            ">67, 167/225, c1=2.941, c2=4.186 g=-0.021\n",
            ">67, 168/225, c1=2.779, c2=7.063 g=-0.021\n",
            ">67, 169/225, c1=-8.358, c2=-6.433 g=-0.021\n",
            ">67, 170/225, c1=-7.455, c2=1.460 g=-0.021\n",
            ">67, 171/225, c1=-8.025, c2=0.819 g=-0.021\n",
            ">67, 172/225, c1=-7.079, c2=-6.170 g=-0.021\n",
            ">67, 173/225, c1=2.149, c2=1.125 g=-0.021\n",
            ">67, 174/225, c1=9.676, c2=11.591 g=-0.021\n",
            ">67, 175/225, c1=-6.310, c2=9.592 g=-0.021\n",
            ">67, 176/225, c1=-8.120, c2=-0.862 g=-0.021\n",
            ">67, 177/225, c1=-1.847, c2=-12.683 g=-0.021\n",
            ">67, 178/225, c1=0.961, c2=-2.226 g=-0.021\n",
            ">67, 179/225, c1=10.059, c2=10.350 g=-0.021\n",
            ">67, 180/225, c1=-6.032, c2=-4.831 g=-0.021\n",
            ">67, 181/225, c1=9.055, c2=3.097 g=-0.021\n",
            ">67, 182/225, c1=-9.489, c2=6.688 g=-0.021\n",
            ">67, 183/225, c1=-6.139, c2=-2.849 g=-0.021\n",
            ">67, 184/225, c1=-2.252, c2=-9.092 g=-0.021\n",
            ">67, 185/225, c1=2.921, c2=-3.102 g=-0.021\n",
            ">67, 186/225, c1=3.292, c2=9.825 g=-0.021\n",
            ">67, 187/225, c1=4.516, c2=-9.578 g=-0.021\n",
            ">67, 188/225, c1=-4.082, c2=-0.037 g=-0.021\n",
            ">67, 189/225, c1=-6.313, c2=-4.224 g=-0.021\n",
            ">67, 190/225, c1=5.883, c2=-14.733 g=-0.021\n",
            ">67, 191/225, c1=2.665, c2=-0.624 g=-0.021\n",
            ">67, 192/225, c1=13.249, c2=1.296 g=-0.021\n",
            ">67, 193/225, c1=-0.169, c2=-1.125 g=-0.021\n",
            ">67, 194/225, c1=-7.953, c2=4.967 g=-0.021\n",
            ">67, 195/225, c1=-2.210, c2=5.070 g=-0.021\n",
            ">67, 196/225, c1=-3.974, c2=7.154 g=-0.021\n",
            ">67, 197/225, c1=2.535, c2=-6.708 g=-0.021\n",
            ">67, 198/225, c1=5.257, c2=1.827 g=-0.021\n",
            ">67, 199/225, c1=-4.962, c2=6.928 g=-0.021\n",
            ">67, 200/225, c1=-4.010, c2=6.268 g=-0.021\n",
            ">67, 201/225, c1=8.474, c2=0.810 g=-0.021\n",
            ">67, 202/225, c1=-7.717, c2=-5.164 g=-0.021\n",
            ">67, 203/225, c1=-9.554, c2=-3.296 g=-0.021\n",
            ">67, 204/225, c1=1.302, c2=-11.112 g=-0.021\n",
            ">67, 205/225, c1=-8.989, c2=-3.473 g=-0.021\n",
            ">67, 206/225, c1=2.189, c2=-3.752 g=-0.021\n",
            ">67, 207/225, c1=3.408, c2=-5.469 g=-0.021\n",
            ">67, 208/225, c1=0.994, c2=-3.373 g=-0.021\n",
            ">67, 209/225, c1=-1.694, c2=5.253 g=-0.021\n",
            ">67, 210/225, c1=1.589, c2=1.140 g=-0.021\n",
            ">67, 211/225, c1=-7.176, c2=-1.884 g=-0.021\n",
            ">67, 212/225, c1=-3.294, c2=11.875 g=-0.021\n",
            ">67, 213/225, c1=-8.043, c2=-13.224 g=-0.021\n",
            ">67, 214/225, c1=-0.540, c2=-9.025 g=-0.021\n",
            ">67, 215/225, c1=-5.105, c2=4.306 g=-0.021\n",
            ">67, 216/225, c1=-0.925, c2=-8.024 g=-0.021\n",
            ">67, 217/225, c1=-5.800, c2=-1.170 g=-0.021\n",
            ">67, 218/225, c1=-8.201, c2=0.387 g=-0.021\n",
            ">67, 219/225, c1=-4.614, c2=5.212 g=-0.021\n",
            ">67, 220/225, c1=10.028, c2=-17.512 g=-0.021\n",
            ">67, 221/225, c1=-9.543, c2=11.545 g=-0.021\n",
            ">67, 222/225, c1=-10.109, c2=-1.738 g=-0.021\n",
            ">67, 223/225, c1=1.251, c2=0.980 g=-0.021\n",
            ">67, 224/225, c1=-3.910, c2=7.800 g=-0.021\n",
            ">67, 225/225, c1=12.983, c2=1.646 g=-0.021\n",
            ">68, 1/225, c1=-2.871, c2=0.546 g=-0.021\n",
            ">68, 2/225, c1=-0.318, c2=1.074 g=-0.021\n",
            ">68, 3/225, c1=2.114, c2=-0.692 g=-0.021\n",
            ">68, 4/225, c1=-10.178, c2=0.224 g=-0.021\n",
            ">68, 5/225, c1=-1.701, c2=-8.633 g=-0.021\n",
            ">68, 6/225, c1=5.384, c2=-4.401 g=-0.021\n",
            ">68, 7/225, c1=5.259, c2=4.553 g=-0.021\n",
            ">68, 8/225, c1=6.596, c2=-2.998 g=-0.021\n",
            ">68, 9/225, c1=8.938, c2=15.912 g=-0.021\n",
            ">68, 10/225, c1=-7.665, c2=9.451 g=-0.021\n",
            ">68, 11/225, c1=2.072, c2=0.336 g=-0.021\n",
            ">68, 12/225, c1=0.176, c2=-0.764 g=-0.021\n",
            ">68, 13/225, c1=-4.094, c2=0.736 g=-0.021\n",
            ">68, 14/225, c1=17.936, c2=-6.010 g=-0.021\n",
            ">68, 15/225, c1=-6.472, c2=2.234 g=-0.021\n",
            ">68, 16/225, c1=3.111, c2=-3.349 g=-0.021\n",
            ">68, 17/225, c1=-8.567, c2=2.021 g=-0.021\n",
            ">68, 18/225, c1=-2.693, c2=7.017 g=-0.021\n",
            ">68, 19/225, c1=1.323, c2=2.682 g=-0.021\n",
            ">68, 20/225, c1=-1.504, c2=7.131 g=-0.021\n",
            ">68, 21/225, c1=-7.689, c2=-0.155 g=-0.021\n",
            ">68, 22/225, c1=-10.794, c2=-10.153 g=-0.021\n",
            ">68, 23/225, c1=2.105, c2=-0.956 g=-0.021\n",
            ">68, 24/225, c1=-7.591, c2=9.174 g=-0.021\n",
            ">68, 25/225, c1=-6.787, c2=5.844 g=-0.021\n",
            ">68, 26/225, c1=-2.229, c2=-2.139 g=-0.021\n",
            ">68, 27/225, c1=3.821, c2=14.655 g=-0.021\n",
            ">68, 28/225, c1=3.211, c2=-8.189 g=-0.021\n",
            ">68, 29/225, c1=-1.423, c2=1.205 g=-0.021\n",
            ">68, 30/225, c1=7.097, c2=5.579 g=-0.021\n",
            ">68, 31/225, c1=-2.377, c2=12.056 g=-0.021\n",
            ">68, 32/225, c1=-8.865, c2=-4.583 g=-0.021\n",
            ">68, 33/225, c1=-11.339, c2=-5.923 g=-0.021\n",
            ">68, 34/225, c1=-5.554, c2=3.419 g=-0.021\n",
            ">68, 35/225, c1=0.323, c2=-3.817 g=-0.021\n",
            ">68, 36/225, c1=1.307, c2=5.274 g=-0.021\n",
            ">68, 37/225, c1=5.888, c2=0.618 g=-0.021\n",
            ">68, 38/225, c1=2.388, c2=-6.139 g=-0.021\n",
            ">68, 39/225, c1=-5.702, c2=2.577 g=-0.021\n",
            ">68, 40/225, c1=-3.916, c2=-1.133 g=-0.021\n",
            ">68, 41/225, c1=4.984, c2=6.652 g=-0.021\n",
            ">68, 42/225, c1=10.137, c2=-9.678 g=-0.021\n",
            ">68, 43/225, c1=-4.169, c2=4.553 g=-0.021\n",
            ">68, 44/225, c1=0.592, c2=-2.079 g=-0.021\n",
            ">68, 45/225, c1=-8.363, c2=4.862 g=-0.021\n",
            ">68, 46/225, c1=-6.741, c2=3.657 g=-0.021\n",
            ">68, 47/225, c1=6.019, c2=-3.731 g=-0.021\n",
            ">68, 48/225, c1=-2.829, c2=1.691 g=-0.021\n",
            ">68, 49/225, c1=-7.634, c2=12.340 g=-0.021\n",
            ">68, 50/225, c1=-10.075, c2=-4.747 g=-0.021\n",
            ">68, 51/225, c1=-3.760, c2=7.558 g=-0.021\n",
            ">68, 52/225, c1=-7.253, c2=5.859 g=-0.021\n",
            ">68, 53/225, c1=-1.771, c2=15.040 g=-0.021\n",
            ">68, 54/225, c1=12.435, c2=-2.111 g=-0.021\n",
            ">68, 55/225, c1=-8.063, c2=-6.127 g=-0.021\n",
            ">68, 56/225, c1=-11.856, c2=3.235 g=-0.021\n",
            ">68, 57/225, c1=-5.115, c2=-6.716 g=-0.021\n",
            ">68, 58/225, c1=-5.206, c2=-7.179 g=-0.021\n",
            ">68, 59/225, c1=-8.524, c2=13.359 g=-0.021\n",
            ">68, 60/225, c1=-4.990, c2=5.036 g=-0.021\n",
            ">68, 61/225, c1=2.278, c2=1.907 g=-0.021\n",
            ">68, 62/225, c1=11.589, c2=-13.099 g=-0.021\n",
            ">68, 63/225, c1=-4.741, c2=-5.672 g=-0.021\n",
            ">68, 64/225, c1=-0.707, c2=-1.356 g=-0.021\n",
            ">68, 65/225, c1=-5.762, c2=10.886 g=-0.021\n",
            ">68, 66/225, c1=-0.217, c2=3.860 g=-0.021\n",
            ">68, 67/225, c1=-8.283, c2=-7.407 g=-0.021\n",
            ">68, 68/225, c1=-0.654, c2=-0.822 g=-0.021\n",
            ">68, 69/225, c1=-10.392, c2=-5.455 g=-0.021\n",
            ">68, 70/225, c1=10.605, c2=-6.254 g=-0.021\n",
            ">68, 71/225, c1=1.255, c2=11.177 g=-0.021\n",
            ">68, 72/225, c1=14.285, c2=7.836 g=-0.021\n",
            ">68, 73/225, c1=1.424, c2=-3.713 g=-0.021\n",
            ">68, 74/225, c1=-3.887, c2=-13.879 g=-0.021\n",
            ">68, 75/225, c1=-0.731, c2=17.259 g=-0.021\n",
            ">68, 76/225, c1=-5.943, c2=8.262 g=-0.021\n",
            ">68, 77/225, c1=3.487, c2=4.273 g=-0.021\n",
            ">68, 78/225, c1=-1.419, c2=-0.726 g=-0.021\n",
            ">68, 79/225, c1=6.945, c2=2.062 g=-0.021\n",
            ">68, 80/225, c1=-1.313, c2=2.362 g=-0.021\n",
            ">68, 81/225, c1=-2.145, c2=-5.202 g=-0.021\n",
            ">68, 82/225, c1=9.511, c2=1.196 g=-0.021\n",
            ">68, 83/225, c1=-2.284, c2=3.749 g=-0.021\n",
            ">68, 84/225, c1=1.788, c2=-9.202 g=-0.021\n",
            ">68, 85/225, c1=-5.399, c2=10.297 g=-0.021\n",
            ">68, 86/225, c1=-6.426, c2=-5.276 g=-0.021\n",
            ">68, 87/225, c1=2.618, c2=0.209 g=-0.021\n",
            ">68, 88/225, c1=-8.693, c2=-2.357 g=-0.021\n",
            ">68, 89/225, c1=4.215, c2=-8.214 g=-0.021\n",
            ">68, 90/225, c1=-9.012, c2=8.638 g=-0.021\n",
            ">68, 91/225, c1=10.650, c2=7.439 g=-0.021\n",
            ">68, 92/225, c1=3.012, c2=7.984 g=-0.021\n",
            ">68, 93/225, c1=1.725, c2=8.003 g=-0.021\n",
            ">68, 94/225, c1=-6.836, c2=7.017 g=-0.021\n",
            ">68, 95/225, c1=12.897, c2=7.884 g=-0.021\n",
            ">68, 96/225, c1=7.815, c2=2.283 g=-0.021\n",
            ">68, 97/225, c1=-0.142, c2=9.534 g=-0.021\n",
            ">68, 98/225, c1=13.250, c2=3.556 g=-0.021\n",
            ">68, 99/225, c1=0.662, c2=-4.589 g=-0.021\n",
            ">68, 100/225, c1=11.553, c2=5.537 g=-0.021\n",
            ">68, 101/225, c1=-2.289, c2=3.454 g=-0.021\n",
            ">68, 102/225, c1=-9.816, c2=-2.572 g=-0.021\n",
            ">68, 103/225, c1=-1.387, c2=3.240 g=-0.021\n",
            ">68, 104/225, c1=-9.585, c2=4.992 g=-0.021\n",
            ">68, 105/225, c1=-5.977, c2=-3.244 g=-0.021\n",
            ">68, 106/225, c1=2.052, c2=-0.786 g=-0.021\n",
            ">68, 107/225, c1=9.431, c2=13.615 g=-0.021\n",
            ">68, 108/225, c1=6.112, c2=7.771 g=-0.021\n",
            ">68, 109/225, c1=-6.994, c2=5.451 g=-0.021\n",
            ">68, 110/225, c1=-2.456, c2=-1.470 g=-0.021\n",
            ">68, 111/225, c1=-0.468, c2=6.363 g=-0.021\n",
            ">68, 112/225, c1=-6.770, c2=-2.082 g=-0.021\n",
            ">68, 113/225, c1=-2.032, c2=6.016 g=-0.021\n",
            ">68, 114/225, c1=-6.009, c2=-2.828 g=-0.021\n",
            ">68, 115/225, c1=-7.643, c2=5.811 g=-0.021\n",
            ">68, 116/225, c1=-3.170, c2=-2.291 g=-0.021\n",
            ">68, 117/225, c1=-0.895, c2=-0.265 g=-0.021\n",
            ">68, 118/225, c1=-11.664, c2=5.560 g=-0.021\n",
            ">68, 119/225, c1=-12.549, c2=5.654 g=-0.021\n",
            ">68, 120/225, c1=-4.357, c2=4.471 g=-0.021\n",
            ">68, 121/225, c1=0.366, c2=-2.261 g=-0.021\n",
            ">68, 122/225, c1=-4.260, c2=14.396 g=-0.021\n",
            ">68, 123/225, c1=10.724, c2=-4.801 g=-0.021\n",
            ">68, 124/225, c1=-4.757, c2=-14.686 g=-0.021\n",
            ">68, 125/225, c1=-6.131, c2=-5.902 g=-0.021\n",
            ">68, 126/225, c1=12.918, c2=5.154 g=-0.021\n",
            ">68, 127/225, c1=-2.167, c2=-0.654 g=-0.021\n",
            ">68, 128/225, c1=4.898, c2=3.465 g=-0.021\n",
            ">68, 129/225, c1=-8.474, c2=-6.567 g=-0.021\n",
            ">68, 130/225, c1=-8.580, c2=5.823 g=-0.021\n",
            ">68, 131/225, c1=8.949, c2=-1.757 g=-0.021\n",
            ">68, 132/225, c1=-6.487, c2=-6.419 g=-0.021\n",
            ">68, 133/225, c1=2.603, c2=0.598 g=-0.021\n",
            ">68, 134/225, c1=-0.388, c2=5.978 g=-0.021\n",
            ">68, 135/225, c1=-7.891, c2=-1.510 g=-0.021\n",
            ">68, 136/225, c1=-0.612, c2=5.276 g=-0.021\n",
            ">68, 137/225, c1=-5.440, c2=3.202 g=-0.021\n",
            ">68, 138/225, c1=21.502, c2=-1.567 g=-0.021\n",
            ">68, 139/225, c1=-0.264, c2=-1.991 g=-0.021\n",
            ">68, 140/225, c1=2.364, c2=1.900 g=-0.021\n",
            ">68, 141/225, c1=12.347, c2=9.205 g=-0.021\n",
            ">68, 142/225, c1=5.825, c2=5.632 g=-0.021\n",
            ">68, 143/225, c1=-4.743, c2=-8.176 g=-0.021\n",
            ">68, 144/225, c1=-2.849, c2=5.946 g=-0.021\n",
            ">68, 145/225, c1=-11.683, c2=0.293 g=-0.021\n",
            ">68, 146/225, c1=-8.164, c2=5.244 g=-0.021\n",
            ">68, 147/225, c1=10.913, c2=3.788 g=-0.021\n",
            ">68, 148/225, c1=2.178, c2=6.285 g=-0.021\n",
            ">68, 149/225, c1=4.503, c2=-0.902 g=-0.021\n",
            ">68, 150/225, c1=11.859, c2=4.571 g=-0.021\n",
            ">68, 151/225, c1=1.396, c2=-3.841 g=-0.021\n",
            ">68, 152/225, c1=-2.288, c2=6.882 g=-0.021\n",
            ">68, 153/225, c1=-5.373, c2=11.408 g=-0.021\n",
            ">68, 154/225, c1=-9.842, c2=-2.385 g=-0.021\n",
            ">68, 155/225, c1=3.730, c2=-0.097 g=-0.021\n",
            ">68, 156/225, c1=8.397, c2=-1.403 g=-0.021\n",
            ">68, 157/225, c1=2.177, c2=2.410 g=-0.021\n",
            ">68, 158/225, c1=-2.728, c2=-1.601 g=-0.021\n",
            ">68, 159/225, c1=-8.780, c2=-2.709 g=-0.021\n",
            ">68, 160/225, c1=-5.854, c2=-7.752 g=-0.021\n",
            ">68, 161/225, c1=-1.292, c2=-0.340 g=-0.021\n",
            ">68, 162/225, c1=3.612, c2=-1.966 g=-0.021\n",
            ">68, 163/225, c1=12.535, c2=-4.495 g=-0.021\n",
            ">68, 164/225, c1=-3.205, c2=-23.409 g=-0.021\n",
            ">68, 165/225, c1=-0.531, c2=3.888 g=-0.021\n",
            ">68, 166/225, c1=11.629, c2=-3.137 g=-0.021\n",
            ">68, 167/225, c1=-0.516, c2=-5.441 g=-0.021\n",
            ">68, 168/225, c1=-3.232, c2=-3.096 g=-0.021\n",
            ">68, 169/225, c1=-1.277, c2=2.351 g=-0.021\n",
            ">68, 170/225, c1=16.480, c2=1.809 g=-0.021\n",
            ">68, 171/225, c1=6.864, c2=-1.928 g=-0.021\n",
            ">68, 172/225, c1=8.657, c2=-4.044 g=-0.021\n",
            ">68, 173/225, c1=-3.348, c2=3.589 g=-0.021\n",
            ">68, 174/225, c1=17.818, c2=4.168 g=-0.021\n",
            ">68, 175/225, c1=-0.540, c2=1.427 g=-0.021\n",
            ">68, 176/225, c1=-6.725, c2=-8.049 g=-0.021\n",
            ">68, 177/225, c1=-2.086, c2=6.285 g=-0.021\n",
            ">68, 178/225, c1=-6.108, c2=-18.019 g=-0.021\n",
            ">68, 179/225, c1=0.166, c2=2.285 g=-0.021\n",
            ">68, 180/225, c1=2.622, c2=-4.191 g=-0.021\n",
            ">68, 181/225, c1=-7.444, c2=-3.743 g=-0.021\n",
            ">68, 182/225, c1=-4.541, c2=9.274 g=-0.021\n",
            ">68, 183/225, c1=-9.099, c2=3.166 g=-0.021\n",
            ">68, 184/225, c1=-6.274, c2=2.691 g=-0.021\n",
            ">68, 185/225, c1=2.244, c2=-2.706 g=-0.021\n",
            ">68, 186/225, c1=5.280, c2=-6.162 g=-0.021\n",
            ">68, 187/225, c1=0.617, c2=-5.628 g=-0.021\n",
            ">68, 188/225, c1=-4.849, c2=5.139 g=-0.021\n",
            ">68, 189/225, c1=-6.388, c2=8.067 g=-0.021\n",
            ">68, 190/225, c1=-5.030, c2=7.118 g=-0.021\n",
            ">68, 191/225, c1=-7.268, c2=-13.895 g=-0.021\n",
            ">68, 192/225, c1=2.498, c2=9.844 g=-0.021\n",
            ">68, 193/225, c1=-2.217, c2=0.745 g=-0.021\n",
            ">68, 194/225, c1=0.202, c2=0.122 g=-0.021\n",
            ">68, 195/225, c1=-10.398, c2=-1.390 g=-0.021\n",
            ">68, 196/225, c1=2.850, c2=0.221 g=-0.021\n",
            ">68, 197/225, c1=9.789, c2=-6.838 g=-0.021\n",
            ">68, 198/225, c1=-0.887, c2=5.419 g=-0.021\n",
            ">68, 199/225, c1=0.469, c2=-9.118 g=-0.021\n",
            ">68, 200/225, c1=9.232, c2=10.642 g=-0.021\n",
            ">68, 201/225, c1=10.680, c2=-2.625 g=-0.021\n",
            ">68, 202/225, c1=6.464, c2=-6.008 g=-0.021\n",
            ">68, 203/225, c1=-2.768, c2=-5.432 g=-0.021\n",
            ">68, 204/225, c1=-4.111, c2=-4.275 g=-0.021\n",
            ">68, 205/225, c1=12.271, c2=1.891 g=-0.021\n",
            ">68, 206/225, c1=-6.233, c2=9.920 g=-0.021\n",
            ">68, 207/225, c1=5.996, c2=-9.104 g=-0.021\n",
            ">68, 208/225, c1=8.524, c2=2.193 g=-0.021\n",
            ">68, 209/225, c1=-5.006, c2=9.349 g=-0.021\n",
            ">68, 210/225, c1=-7.191, c2=1.566 g=-0.021\n",
            ">68, 211/225, c1=-5.902, c2=-1.990 g=-0.021\n",
            ">68, 212/225, c1=-4.818, c2=11.090 g=-0.021\n",
            ">68, 213/225, c1=-6.026, c2=-0.860 g=-0.021\n",
            ">68, 214/225, c1=5.469, c2=-3.331 g=-0.021\n",
            ">68, 215/225, c1=3.583, c2=-5.958 g=-0.021\n",
            ">68, 216/225, c1=6.507, c2=-9.290 g=-0.021\n",
            ">68, 217/225, c1=-0.477, c2=-5.220 g=-0.021\n",
            ">68, 218/225, c1=-3.557, c2=3.114 g=-0.021\n",
            ">68, 219/225, c1=6.408, c2=3.288 g=-0.021\n",
            ">68, 220/225, c1=17.795, c2=0.483 g=-0.021\n",
            ">68, 221/225, c1=-8.545, c2=-6.098 g=-0.021\n",
            ">68, 222/225, c1=0.284, c2=-1.721 g=-0.021\n",
            ">68, 223/225, c1=1.346, c2=6.588 g=-0.021\n",
            ">68, 224/225, c1=-6.809, c2=-0.223 g=-0.021\n",
            ">68, 225/225, c1=-4.145, c2=-9.078 g=-0.021\n",
            "Model_epoch_68 saved\n",
            ">69, 1/225, c1=6.916, c2=-5.416 g=-0.021\n",
            ">69, 2/225, c1=-2.841, c2=-7.831 g=-0.021\n",
            ">69, 3/225, c1=-3.182, c2=-4.432 g=-0.021\n",
            ">69, 4/225, c1=0.118, c2=-5.464 g=-0.021\n",
            ">69, 5/225, c1=9.870, c2=-13.577 g=-0.021\n",
            ">69, 6/225, c1=-1.783, c2=-6.940 g=-0.021\n",
            ">69, 7/225, c1=1.850, c2=-3.817 g=-0.021\n",
            ">69, 8/225, c1=-12.033, c2=-9.475 g=-0.021\n",
            ">69, 9/225, c1=1.259, c2=3.819 g=-0.021\n",
            ">69, 10/225, c1=10.320, c2=-2.896 g=-0.021\n",
            ">69, 11/225, c1=13.518, c2=0.947 g=-0.021\n",
            ">69, 12/225, c1=3.038, c2=-3.898 g=-0.021\n",
            ">69, 13/225, c1=9.792, c2=-5.864 g=-0.021\n",
            ">69, 14/225, c1=14.560, c2=2.472 g=-0.021\n",
            ">69, 15/225, c1=-14.354, c2=-0.939 g=-0.021\n",
            ">69, 16/225, c1=6.643, c2=-2.874 g=-0.021\n",
            ">69, 17/225, c1=8.624, c2=1.685 g=-0.021\n",
            ">69, 18/225, c1=10.722, c2=8.576 g=-0.021\n",
            ">69, 19/225, c1=-5.687, c2=-6.378 g=-0.021\n",
            ">69, 20/225, c1=9.681, c2=7.702 g=-0.021\n",
            ">69, 21/225, c1=-4.611, c2=-2.264 g=-0.021\n",
            ">69, 22/225, c1=13.663, c2=0.871 g=-0.021\n",
            ">69, 23/225, c1=-6.879, c2=-4.247 g=-0.021\n",
            ">69, 24/225, c1=8.006, c2=-11.373 g=-0.021\n",
            ">69, 25/225, c1=9.531, c2=6.162 g=-0.021\n",
            ">69, 26/225, c1=4.833, c2=8.348 g=-0.021\n",
            ">69, 27/225, c1=-4.921, c2=15.108 g=-0.021\n",
            ">69, 28/225, c1=1.252, c2=19.413 g=-0.021\n",
            ">69, 29/225, c1=6.546, c2=-4.212 g=-0.021\n",
            ">69, 30/225, c1=4.528, c2=-9.055 g=-0.021\n",
            ">69, 31/225, c1=0.839, c2=4.810 g=-0.021\n",
            ">69, 32/225, c1=-6.234, c2=-0.782 g=-0.021\n",
            ">69, 33/225, c1=-4.003, c2=-1.696 g=-0.021\n",
            ">69, 34/225, c1=11.619, c2=-1.106 g=-0.021\n",
            ">69, 35/225, c1=-12.096, c2=12.614 g=-0.021\n",
            ">69, 36/225, c1=2.240, c2=20.134 g=-0.021\n",
            ">69, 37/225, c1=-1.108, c2=-4.632 g=-0.021\n",
            ">69, 38/225, c1=-3.371, c2=3.708 g=-0.021\n",
            ">69, 39/225, c1=-2.383, c2=1.237 g=-0.021\n",
            ">69, 40/225, c1=8.191, c2=-5.496 g=-0.021\n",
            ">69, 41/225, c1=-8.291, c2=-3.747 g=-0.021\n",
            ">69, 42/225, c1=15.423, c2=9.533 g=-0.021\n",
            ">69, 43/225, c1=-10.542, c2=-10.336 g=-0.021\n",
            ">69, 44/225, c1=9.510, c2=-5.080 g=-0.021\n",
            ">69, 45/225, c1=4.419, c2=0.196 g=-0.021\n",
            ">69, 46/225, c1=0.335, c2=-1.807 g=-0.021\n",
            ">69, 47/225, c1=2.832, c2=5.280 g=-0.021\n",
            ">69, 48/225, c1=2.593, c2=-5.522 g=-0.021\n",
            ">69, 49/225, c1=17.641, c2=-2.281 g=-0.021\n",
            ">69, 50/225, c1=-0.769, c2=3.188 g=-0.021\n",
            ">69, 51/225, c1=1.775, c2=-1.621 g=-0.021\n",
            ">69, 52/225, c1=1.844, c2=11.801 g=-0.021\n",
            ">69, 53/225, c1=2.848, c2=5.991 g=-0.021\n",
            ">69, 54/225, c1=-9.269, c2=7.519 g=-0.021\n",
            ">69, 55/225, c1=-3.433, c2=-5.812 g=-0.021\n",
            ">69, 56/225, c1=-4.670, c2=-2.621 g=-0.021\n",
            ">69, 57/225, c1=-6.297, c2=3.292 g=-0.021\n",
            ">69, 58/225, c1=-8.727, c2=-3.443 g=-0.021\n",
            ">69, 59/225, c1=3.165, c2=-0.793 g=-0.021\n",
            ">69, 60/225, c1=15.079, c2=-3.024 g=-0.021\n",
            ">69, 61/225, c1=2.252, c2=-0.836 g=-0.021\n",
            ">69, 62/225, c1=9.024, c2=-5.698 g=-0.021\n",
            ">69, 63/225, c1=3.484, c2=-5.100 g=-0.021\n",
            ">69, 64/225, c1=-3.089, c2=5.287 g=-0.021\n",
            ">69, 65/225, c1=3.772, c2=0.700 g=-0.021\n",
            ">69, 66/225, c1=-0.138, c2=-7.421 g=-0.021\n",
            ">69, 67/225, c1=7.573, c2=-1.199 g=-0.021\n",
            ">69, 68/225, c1=2.360, c2=-3.462 g=-0.021\n",
            ">69, 69/225, c1=-4.860, c2=1.378 g=-0.021\n",
            ">69, 70/225, c1=3.791, c2=1.835 g=-0.021\n",
            ">69, 71/225, c1=-0.811, c2=-6.127 g=-0.021\n",
            ">69, 72/225, c1=-6.908, c2=-5.443 g=-0.021\n",
            ">69, 73/225, c1=4.402, c2=5.113 g=-0.021\n",
            ">69, 74/225, c1=-7.860, c2=-1.917 g=-0.021\n",
            ">69, 75/225, c1=-6.592, c2=-5.779 g=-0.021\n",
            ">69, 76/225, c1=1.362, c2=-7.300 g=-0.021\n",
            ">69, 77/225, c1=12.895, c2=3.140 g=-0.021\n",
            ">69, 78/225, c1=15.212, c2=-0.974 g=-0.021\n",
            ">69, 79/225, c1=-9.421, c2=-18.505 g=-0.021\n",
            ">69, 80/225, c1=15.855, c2=-2.653 g=-0.021\n",
            ">69, 81/225, c1=1.455, c2=-0.459 g=-0.021\n",
            ">69, 82/225, c1=-0.030, c2=-0.568 g=-0.021\n",
            ">69, 83/225, c1=9.641, c2=7.993 g=-0.021\n",
            ">69, 84/225, c1=13.387, c2=3.828 g=-0.021\n",
            ">69, 85/225, c1=10.683, c2=-4.162 g=-0.021\n",
            ">69, 86/225, c1=-7.048, c2=3.147 g=-0.021\n",
            ">69, 87/225, c1=-9.618, c2=-3.109 g=-0.021\n",
            ">69, 88/225, c1=-7.418, c2=3.244 g=-0.021\n",
            ">69, 89/225, c1=-5.867, c2=1.338 g=-0.021\n",
            ">69, 90/225, c1=5.886, c2=6.518 g=-0.021\n",
            ">69, 91/225, c1=-3.796, c2=6.926 g=-0.021\n",
            ">69, 92/225, c1=6.076, c2=-0.773 g=-0.021\n",
            ">69, 93/225, c1=-7.561, c2=-3.224 g=-0.021\n",
            ">69, 94/225, c1=-3.345, c2=6.106 g=-0.021\n",
            ">69, 95/225, c1=1.185, c2=-2.966 g=-0.021\n",
            ">69, 96/225, c1=7.111, c2=-1.471 g=-0.021\n",
            ">69, 97/225, c1=15.936, c2=-0.890 g=-0.021\n",
            ">69, 98/225, c1=5.988, c2=15.449 g=-0.021\n",
            ">69, 99/225, c1=13.512, c2=4.308 g=-0.021\n",
            ">69, 100/225, c1=-6.168, c2=12.617 g=-0.021\n",
            ">69, 101/225, c1=-7.474, c2=3.572 g=-0.021\n",
            ">69, 102/225, c1=-6.390, c2=-3.495 g=-0.021\n",
            ">69, 103/225, c1=-5.469, c2=-2.390 g=-0.021\n",
            ">69, 104/225, c1=-0.207, c2=-0.256 g=-0.021\n",
            ">69, 105/225, c1=6.779, c2=7.530 g=-0.021\n",
            ">69, 106/225, c1=-1.247, c2=0.225 g=-0.021\n",
            ">69, 107/225, c1=7.381, c2=-7.085 g=-0.021\n",
            ">69, 108/225, c1=-2.572, c2=4.348 g=-0.021\n",
            ">69, 109/225, c1=-3.357, c2=15.162 g=-0.021\n",
            ">69, 110/225, c1=-0.203, c2=14.558 g=-0.021\n",
            ">69, 111/225, c1=0.341, c2=8.847 g=-0.021\n",
            ">69, 112/225, c1=-7.685, c2=3.364 g=-0.021\n",
            ">69, 113/225, c1=-5.193, c2=1.359 g=-0.021\n",
            ">69, 114/225, c1=1.374, c2=11.945 g=-0.021\n",
            ">69, 115/225, c1=-9.085, c2=2.202 g=-0.021\n",
            ">69, 116/225, c1=-5.764, c2=-1.832 g=-0.021\n",
            ">69, 117/225, c1=4.439, c2=-15.851 g=-0.021\n",
            ">69, 118/225, c1=4.314, c2=-4.203 g=-0.021\n",
            ">69, 119/225, c1=1.830, c2=-0.457 g=-0.021\n",
            ">69, 120/225, c1=-3.431, c2=1.436 g=-0.021\n",
            ">69, 121/225, c1=-5.768, c2=-6.390 g=-0.021\n",
            ">69, 122/225, c1=2.170, c2=-1.542 g=-0.021\n",
            ">69, 123/225, c1=-0.701, c2=-4.035 g=-0.021\n",
            ">69, 124/225, c1=15.544, c2=0.223 g=-0.021\n",
            ">69, 125/225, c1=-0.236, c2=-3.718 g=-0.021\n",
            ">69, 126/225, c1=8.694, c2=-6.551 g=-0.021\n",
            ">69, 127/225, c1=3.442, c2=-3.420 g=-0.021\n",
            ">69, 128/225, c1=-6.874, c2=-0.784 g=-0.021\n",
            ">69, 129/225, c1=5.675, c2=9.561 g=-0.021\n",
            ">69, 130/225, c1=-5.849, c2=-1.155 g=-0.021\n",
            ">69, 131/225, c1=9.426, c2=-0.778 g=-0.021\n",
            ">69, 132/225, c1=-6.390, c2=5.602 g=-0.021\n",
            ">69, 133/225, c1=-8.182, c2=-3.370 g=-0.021\n",
            ">69, 134/225, c1=13.529, c2=2.973 g=-0.021\n",
            ">69, 135/225, c1=-1.674, c2=-2.940 g=-0.021\n",
            ">69, 136/225, c1=-2.342, c2=9.031 g=-0.021\n",
            ">69, 137/225, c1=-4.061, c2=1.756 g=-0.021\n",
            ">69, 138/225, c1=-2.968, c2=-4.869 g=-0.021\n",
            ">69, 139/225, c1=-12.815, c2=-0.596 g=-0.021\n",
            ">69, 140/225, c1=-8.772, c2=-8.763 g=-0.021\n",
            ">69, 141/225, c1=12.970, c2=6.410 g=-0.021\n",
            ">69, 142/225, c1=12.486, c2=4.848 g=-0.021\n",
            ">69, 143/225, c1=-10.947, c2=-6.216 g=-0.021\n",
            ">69, 144/225, c1=3.871, c2=2.676 g=-0.021\n",
            ">69, 145/225, c1=-2.686, c2=4.469 g=-0.021\n",
            ">69, 146/225, c1=-6.737, c2=-6.258 g=-0.021\n",
            ">69, 147/225, c1=-9.555, c2=1.937 g=-0.021\n",
            ">69, 148/225, c1=16.784, c2=-6.850 g=-0.021\n",
            ">69, 149/225, c1=-0.731, c2=-3.427 g=-0.021\n",
            ">69, 150/225, c1=-2.197, c2=-1.687 g=-0.021\n",
            ">69, 151/225, c1=-11.975, c2=-3.465 g=-0.021\n",
            ">69, 152/225, c1=-8.085, c2=-14.476 g=-0.021\n",
            ">69, 153/225, c1=-5.195, c2=-11.682 g=-0.021\n",
            ">69, 154/225, c1=4.084, c2=-4.930 g=-0.021\n",
            ">69, 155/225, c1=-9.638, c2=-5.551 g=-0.021\n",
            ">69, 156/225, c1=8.418, c2=-3.077 g=-0.021\n",
            ">69, 157/225, c1=1.382, c2=-0.082 g=-0.021\n",
            ">69, 158/225, c1=-8.996, c2=-6.370 g=-0.021\n",
            ">69, 159/225, c1=6.870, c2=-0.081 g=-0.021\n",
            ">69, 160/225, c1=-7.412, c2=-14.854 g=-0.021\n",
            ">69, 161/225, c1=-1.401, c2=2.011 g=-0.021\n",
            ">69, 162/225, c1=-5.301, c2=1.620 g=-0.021\n",
            ">69, 163/225, c1=3.603, c2=0.077 g=-0.021\n",
            ">69, 164/225, c1=0.713, c2=4.839 g=-0.021\n",
            ">69, 165/225, c1=6.593, c2=-1.808 g=-0.021\n",
            ">69, 166/225, c1=1.695, c2=9.099 g=-0.021\n",
            ">69, 167/225, c1=8.400, c2=-12.641 g=-0.021\n",
            ">69, 168/225, c1=-7.910, c2=-4.455 g=-0.021\n",
            ">69, 169/225, c1=9.722, c2=-4.155 g=-0.021\n",
            ">69, 170/225, c1=-9.866, c2=2.385 g=-0.021\n",
            ">69, 171/225, c1=-0.151, c2=6.136 g=-0.021\n",
            ">69, 172/225, c1=-1.678, c2=1.577 g=-0.021\n",
            ">69, 173/225, c1=5.504, c2=-7.964 g=-0.021\n",
            ">69, 174/225, c1=-1.038, c2=6.128 g=-0.021\n",
            ">69, 175/225, c1=2.315, c2=-8.042 g=-0.021\n",
            ">69, 176/225, c1=-9.324, c2=3.509 g=-0.021\n",
            ">69, 177/225, c1=1.206, c2=9.619 g=-0.021\n",
            ">69, 178/225, c1=-8.966, c2=-10.712 g=-0.021\n",
            ">69, 179/225, c1=-8.434, c2=-3.939 g=-0.021\n",
            ">69, 180/225, c1=-11.655, c2=-10.458 g=-0.022\n",
            ">69, 181/225, c1=-9.213, c2=-5.602 g=-0.022\n",
            ">69, 182/225, c1=3.895, c2=1.046 g=-0.022\n",
            ">69, 183/225, c1=-8.092, c2=-9.282 g=-0.022\n",
            ">69, 184/225, c1=7.720, c2=-16.586 g=-0.022\n",
            ">69, 185/225, c1=-10.293, c2=10.891 g=-0.022\n",
            ">69, 186/225, c1=0.126, c2=-6.625 g=-0.022\n",
            ">69, 187/225, c1=-2.945, c2=5.866 g=-0.022\n",
            ">69, 188/225, c1=10.969, c2=-7.335 g=-0.022\n",
            ">69, 189/225, c1=1.426, c2=-3.296 g=-0.022\n",
            ">69, 190/225, c1=-4.258, c2=9.218 g=-0.022\n",
            ">69, 191/225, c1=6.220, c2=2.419 g=-0.022\n",
            ">69, 192/225, c1=-7.841, c2=0.791 g=-0.022\n",
            ">69, 193/225, c1=-6.361, c2=5.795 g=-0.022\n",
            ">69, 194/225, c1=-3.874, c2=-3.510 g=-0.022\n",
            ">69, 195/225, c1=-1.461, c2=-2.022 g=-0.022\n",
            ">69, 196/225, c1=-3.478, c2=-18.348 g=-0.022\n",
            ">69, 197/225, c1=-9.348, c2=-10.574 g=-0.022\n",
            ">69, 198/225, c1=1.275, c2=8.661 g=-0.022\n",
            ">69, 199/225, c1=-10.297, c2=-1.719 g=-0.022\n",
            ">69, 200/225, c1=-8.613, c2=-2.016 g=-0.022\n",
            ">69, 201/225, c1=-7.202, c2=-14.166 g=-0.022\n",
            ">69, 202/225, c1=-7.839, c2=-0.398 g=-0.022\n",
            ">69, 203/225, c1=-4.438, c2=-1.553 g=-0.022\n",
            ">69, 204/225, c1=-3.511, c2=-1.585 g=-0.022\n",
            ">69, 205/225, c1=-6.874, c2=-4.160 g=-0.022\n",
            ">69, 206/225, c1=-1.778, c2=2.327 g=-0.022\n",
            ">69, 207/225, c1=-4.186, c2=-6.340 g=-0.022\n",
            ">69, 208/225, c1=-7.458, c2=10.682 g=-0.022\n",
            ">69, 209/225, c1=-4.720, c2=1.851 g=-0.022\n",
            ">69, 210/225, c1=3.358, c2=-1.907 g=-0.022\n",
            ">69, 211/225, c1=-2.388, c2=5.226 g=-0.022\n",
            ">69, 212/225, c1=-4.329, c2=-0.737 g=-0.022\n",
            ">69, 213/225, c1=13.910, c2=-2.666 g=-0.022\n",
            ">69, 214/225, c1=15.020, c2=2.817 g=-0.022\n",
            ">69, 215/225, c1=5.442, c2=-3.252 g=-0.022\n",
            ">69, 216/225, c1=2.902, c2=-5.873 g=-0.022\n",
            ">69, 217/225, c1=-1.908, c2=-5.035 g=-0.022\n",
            ">69, 218/225, c1=-9.371, c2=-7.909 g=-0.022\n",
            ">69, 219/225, c1=0.177, c2=11.140 g=-0.022\n",
            ">69, 220/225, c1=22.708, c2=-7.487 g=-0.022\n",
            ">69, 221/225, c1=0.658, c2=6.874 g=-0.022\n",
            ">69, 222/225, c1=-3.998, c2=4.048 g=-0.022\n",
            ">69, 223/225, c1=-7.581, c2=-8.638 g=-0.022\n",
            ">69, 224/225, c1=-2.020, c2=2.375 g=-0.022\n",
            ">69, 225/225, c1=-4.050, c2=-0.938 g=-0.022\n",
            ">70, 1/225, c1=2.200, c2=-12.372 g=-0.022\n",
            ">70, 2/225, c1=-8.321, c2=2.769 g=-0.022\n",
            ">70, 3/225, c1=11.398, c2=-3.353 g=-0.022\n",
            ">70, 4/225, c1=-7.184, c2=-1.781 g=-0.022\n",
            ">70, 5/225, c1=-12.544, c2=7.506 g=-0.022\n",
            ">70, 6/225, c1=6.223, c2=1.174 g=-0.022\n",
            ">70, 7/225, c1=-5.666, c2=-1.100 g=-0.022\n",
            ">70, 8/225, c1=-6.369, c2=2.065 g=-0.022\n",
            ">70, 9/225, c1=-0.467, c2=-10.445 g=-0.022\n",
            ">70, 10/225, c1=-5.768, c2=-3.063 g=-0.022\n",
            ">70, 11/225, c1=-7.636, c2=-8.342 g=-0.022\n",
            ">70, 12/225, c1=-1.018, c2=8.232 g=-0.022\n",
            ">70, 13/225, c1=-9.170, c2=3.637 g=-0.022\n",
            ">70, 14/225, c1=-9.977, c2=-14.934 g=-0.022\n",
            ">70, 15/225, c1=1.387, c2=-11.473 g=-0.022\n",
            ">70, 16/225, c1=5.897, c2=3.014 g=-0.022\n",
            ">70, 17/225, c1=9.568, c2=3.452 g=-0.022\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c7c2ea9ba8f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-9277519bb1fb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(resume_training, initial_epoch, latent_dim)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m   train_wgan(g_model, c_model, gan_model, dataset, latent_dim,\n\u001b[0;32m---> 54\u001b[0;31m              csv_path=csv_path, n_epochs=400, initial_epoch=initial_epoch)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-6f5661bece07>\u001b[0m in \u001b[0;36mtrain_wgan\u001b[0;34m(g_model, c_model, gan_model, dataset, latent_dim, csv_path, n_epochs, n_batch, n_critic, initial_epoch)\u001b[0m\n\u001b[1;32m    256\u001b[0m                         \u001b[0mc1_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_loss1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                         \u001b[0;31m# generate 'fake' examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                         \u001b[0mX_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_fake_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalf_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m                         \u001b[0;31m# update critic model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                         \u001b[0mc_loss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-6f5661bece07>\u001b[0m in \u001b[0;36mgenerate_fake_samples\u001b[0;34m(generator, latent_dim, n_samples, label_noising, p_flip)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mx_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_latent_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# predict outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;31m# create class labels with 1.0 for 'fake'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oahRtfoqiIJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "last_step = max([int(re.findall('[0-9]+',s.split('.')[0])[0]) for s in os.listdir('model_checkpoints')])\n",
        "last_epoch = max([int(re.findall('[0-9]+',s.split('.')[0])[0]) for s in os.listdir('model_checkpoints')])\n",
        "\n",
        "model_paths = [\"generator_model_%03d.h5\",\"critic_model_weights_%03d.h5\",\n",
        "              \"gan_model_weights_%03d.h5\",\"gan_optimizer_weights_%03d.npy\"]\n",
        "model_filenames = [os.path.join(\"model_checkpoints\", p % (last_epoch))  for p in model_paths]  \n",
        "print(model_filenames, last_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhwsiXPRGJnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g_model = load_model(os.path.join('model_checkpoints','generator_model_015.h5'))\n",
        "latent_dim = 250\n",
        "n_samples = 100\n",
        "X, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "# scale from [-1,1] to [0,1]\n",
        "X = (X + 1) / 2.0\n",
        "# plot images\n",
        "print(X.shape)\n",
        "for i in range(7 * 7):\n",
        "  # define subplot\n",
        "  pyplot.subplot(7, 7, 1 + i)\n",
        "  # turn off axis\n",
        "  pyplot.axis('off')\n",
        "  # plot raw pixel data\n",
        "  pyplot.imshow(X[i,:,:,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mngk45GKIrwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "df = pd.read_csv('loss.csv', index_col=None)\n",
        "# print(df[df.columns].loc[df['steps']<=225])\n",
        "# df.insert(0, 'epoch', '')\n",
        "# df['epoch'] = df['steps'].map(lambda x: math.ceil(x/225))\n",
        "# df['steps'] = df['steps'].map(lambda x: \"%d/%d\" % ((x%225) if (x%225)!=0 else 225, 225))\n",
        "# df.to_csv('loss.csv', index=False)\n",
        "# df = df.loc[df['epoch'] <= 12 ]\n",
        "# df = df.drop(df.index[-1])\n",
        "# df.to_csv('loss.csv', index=False)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHoZi3CacLRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# os.chdir('figure_plots')\n",
        "# print(os.getcwd())\n",
        "# for plot in os.listdir('.'):\n",
        "#   steps = int(re.findall('[0-9]+',plot)[0])\n",
        "#   new_fname = \"generated_plot_%03d.png\" % (steps/225)\n",
        "#   os.rename(plot, new_fname)\n",
        "#   print(plot,new_fname)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr4wL-T9eUOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_models = filter(lambda x: x,[re.findall('[0-9]+',s.split('.')[0]) for s in os.listdir('model_checkpoints')])\n",
        "last_epoch = max([int(g[0]) for g in generator_models])\n",
        "last_epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7WtnF6S-kAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a = [a*225 for a in range(1,10)]\n",
        "# for i in a:\n",
        "#   if i % (225*2) == 0:\n",
        "#     print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQJ8qfjQIKN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}